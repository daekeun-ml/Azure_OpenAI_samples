{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Text To Speech] Synthetic Audio Dataset Creation\n",
    "\n",
    "This sample demonstrates how to use Azure AI Speech API to generate synthetic audio dataset from text.\n",
    "\n",
    "> âœ¨ **_Note_** <br>\n",
    "> Please check the supported languages and region availabilty before you get started - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts / https://learn.microsoft.com/en-us/azure/ai-services/speech-service/regions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Configure a Python virtual environment for 3.10 or later:\n",
    "\n",
    "1.  open the Command Palette (Ctrl+Shift+P).\n",
    "1.  Search for Python: Create Environment.\n",
    "1.  select Venv / Conda and choose where to create the new environment.\n",
    "1.  Select the Python interpreter version. Create with version 3.10 or later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## 1. Test Text to Speech Using the Speech SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "SPEECH_KEY = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "SPEECH_REGION = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "CUSTOM_SPEECH_LANG = os.getenv(\"CUSTOM_SPEECH_LANG\")\n",
    "CUSTOM_SPEECH_LOCALE = os.getenv(\"CUSTOM_SPEECH_LOCALE\")\n",
    "TTS_FOR_TRAIN = os.getenv(\"TTS_FOR_TRAIN\")\n",
    "TTS_FOR_EVAL = os.getenv(\"TTS_FOR_EVAL\")\n",
    "\n",
    "synthetic_text_file = \"\"\n",
    "%store -r synthetic_text_file\n",
    "try:\n",
    "    synthetic_text_file\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the previous notebook again.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "print(\"=== Azure AI Speech Info ===\")\n",
    "print(f\"SPEECH_REGION={SPEECH_REGION}\")\n",
    "print(f\"CUSTOM_SPEECH_LANG={CUSTOM_SPEECH_LANG}\")\n",
    "print(f\"CUSTOM_SPEECH_LOCALE={CUSTOM_SPEECH_LOCALE}\")  \n",
    "print(f\"TTS_FOR_TRAIN={TTS_FOR_TRAIN}\") \n",
    "print(f\"TTS_FOR_EVAL={TTS_FOR_EVAL}\") \n",
    "print(f\"Synthetic Text File={synthetic_text_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of a speech config with specified subscription key and service region.\n",
    "Replace with your own subscription key and service region (e.g., \"westus\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type some text that you want to speak...\")\n",
    "text = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Speak text and Store the output stream to a wav file (speak_text_async)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RETRIES = 2\n",
    "for _ in range(MIN_RETRIES):\n",
    "    try:\n",
    "        result = speech_synthesizer.speak_text_async(text).get()\n",
    "    except Exception as e:\n",
    "        time.sleep(10)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"sample\"\n",
    "    \n",
    "if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized to speaker for text [{}]\".format(text))\n",
    "    stream = speechsdk.AudioDataStream(result)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    stream.save_to_wav_file(os.path.join(output_dir, \"result_text.wav\"))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        if cancellation_details.error_details:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    print(\"Did you update the subscription info?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Speak SSML and Store the output stream to a wav file (speak_ssml_async)\n",
    "\n",
    "-   To get TTS voice id for specific languages, please check the voice gallery for more options, https://speech.microsoft.com/portal?projecttype=voicegallery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tts_voice = 'en-US-JennyMultilingualV2Neural' # Default TTS voice for English, To get TTS voice id for specific languages, please check the voice gallery for more options\n",
    "lang = \"en-US\"\n",
    "\n",
    "ssml = f\"\"\"<speak version='1.0'  xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang='{lang}'>\n",
    "                     <voice name='{default_tts_voice}'>\n",
    "                             {html.escape(text)}\n",
    "                     </voice>\n",
    "                   </speak>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "speech_sythesis_result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "stream = speechsdk.AudioDataStream(speech_sythesis_result)\n",
    "stream.save_to_wav_file(os.path.join(output_dir,\"result_ssml.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate syntethic dataset and manifest.txt file as a training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the ssml template for the dataset\n",
    "\n",
    "-   The ssml template is used to generate the synthetic dataset. Here is the reference for the ssml template: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-synthesis-markup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_file_by_speech_synthesis(text, file_path, lang, default_tts_voice):\n",
    "    ssml = f\"\"\"<speak version='1.0'  xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang='{lang}'>\n",
    "                     <voice name='{default_tts_voice}'>\n",
    "                             {html.escape(text)}\n",
    "                     </voice>\n",
    "                   </speak>\"\"\"\n",
    "    speech_sythesis_result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "    stream = speechsdk.AudioDataStream(speech_sythesis_result)\n",
    "    stream.save_to_wav_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic wav dataset and manifest for Specific language from the synthetic_text_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Check https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt for supported locale\n",
    "language = CUSTOM_SPEECH_LOCALE\n",
    "\n",
    "output_dir = \"synthetic_data\"\n",
    "DELETE_OLD_DATA = True\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if(DELETE_OLD_DATA):\n",
    "    for file in os.listdir(output_dir):\n",
    "        os.remove(os.path.join(output_dir, file))    \n",
    "\n",
    "train_tts_voices = TTS_FOR_TRAIN.split(',')\n",
    "\n",
    "for tts_voice in train_tts_voices:\n",
    "    with open(synthetic_text_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                expression = json.loads(line)\n",
    "                no = expression['no']\n",
    "                text = expression[language]\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                file_name = f\"{no}_locale_{language}_speaker_{tts_voice}_{timestamp}.wav\"\n",
    "                print(f\"Generating {file_name}\")\n",
    "                get_audio_file_by_speech_synthesis(text, os.path.join(output_dir,file_name), language, tts_voice)\n",
    "                with open(f'{output_dir}/manifest.txt', 'a', encoding='utf-8') as manifest_file:\n",
    "                    manifest_file.write(f\"{file_name}\\t{text}\\n\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play WAV Files to test result in Output Folder\n",
    "\n",
    "-   Use the os library to list all WAV files in the output folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "files = os.listdir(output_dir)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play WAV Files\n",
    "\n",
    "-   Use IPython.display.Audio to play each WAV file listed in the output folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play each WAV file in the output folder\n",
    "for wav_file in wav_files[:3]:\n",
    "    file_path = os.path.join(output_dir, wav_file)\n",
    "    display(Audio(filename=file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio Data Augmentation\n",
    "\n",
    "In this section, we will give various modulations to the synthetic data through audio data augmentation.\n",
    "\n",
    "[Audiomentations](https://github.com/iver56/audiomentations) is a Python library for audio data augmentation, commonly used in machine learning projects to improve model robustness by creating variations in training data. It provides a range of transforms like adding noise, time stretching, pitch shifting, and applying reverb. The library is designed for ease of use, allowing users to apply multiple transformations to audio samples with customizable parameters. Audiomentations supports both mono and stereo audio and integrates seamlessly with common audio processing workflows. It's lightweight, efficient, and helps simulate real-world audio conditions for better generalization in models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import (\n",
    "    AddBackgroundNoise, OneOf, Compose, Aliasing, AddGaussianNoise, LoudnessNormalization, TimeStretch, PitchShift, Shift, Gain, GainTransition, \n",
    "    BandPassFilter, BandStopFilter, AddGaussianSNR, AddColorNoise, LowPassFilter, LowShelfFilter, HighPassFilter, HighShelfFilter, TimeStretch,\n",
    "    PitchShift, Shift, AdjustDuration, ClippingDistortion, AirAbsorption, PeakingFilter, Normalize\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "augment = Compose([\n",
    "    OneOf([\n",
    "        AddBackgroundNoise(\n",
    "            sounds_path=\"noise_sample/bg-noise.mp3\",\n",
    "            noise_rms=\"absolute\",\n",
    "            min_absolute_rms_db=-30,\n",
    "            max_absolute_rms_db=-10,\n",
    "        ),\n",
    "        AddBackgroundNoise(\n",
    "            sounds_path=\"noise_sample/bg-noise.mp3\",\n",
    "            min_snr_db=2,\n",
    "            max_snr_db=4,\n",
    "        ),\n",
    "    ], p=0.3),      \n",
    "    OneOf([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1.0),   \n",
    "        AddGaussianSNR(min_snr_db=5.0, max_snr_db=40.0, p=1.0),\n",
    "        LoudnessNormalization(p=1.0),\n",
    "        Aliasing(p=1.0) \n",
    "    ], p=0.3),\n",
    "    OneOf([\n",
    "        LowPassFilter(p=1.0),\n",
    "        LowShelfFilter(p=1.0),\n",
    "        HighPassFilter(p=1.0),\n",
    "        HighShelfFilter(p=1.0),\n",
    "        BandPassFilter(p=1.0),\n",
    "        BandStopFilter(p=1.0),\n",
    "        ClippingDistortion(p=0.8),\n",
    "        AirAbsorption(p=0.8),\n",
    "        PeakingFilter(p=0.8)\n",
    "    ], p=0.6),\n",
    "    OneOf([\n",
    "        Gain(min_gain_db=-6.0, max_gain_db=6.0, p=1.0),\n",
    "        GainTransition(p=1.0),        \n",
    "        AdjustDuration(duration_seconds=5.0, p=0.5),\n",
    "        AdjustDuration(duration_seconds=5.0, padding_position=\"start\", padding_mode=\"wrap\", p=0.5),\n",
    "        AdjustDuration(duration_seconds=5.0, padding_position=\"start\", padding_mode=\"reflect\", p=0.5),\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "        Shift(p=0.5),\n",
    "    ], p=0.3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from audiomentations.core.audio_loading_utils import load_sound_file\n",
    "\n",
    "output_dir_aug = \"synthetic_data_aug\"\n",
    "NUM_AUGMENTS = 5\n",
    "\n",
    "if not os.path.exists(output_dir_aug):\n",
    "    os.makedirs(output_dir_aug)\n",
    "\n",
    "\n",
    "# Play each WAV file in the output folder\n",
    "for wav_file in wav_files:\n",
    "    file_path = os.path.join(output_dir, wav_file)\n",
    "    samples, sample_rate = load_sound_file(file_path, sample_rate=None, mono=False)\n",
    "    \n",
    "    if len(samples.shape) == 2 and samples.shape[0] > samples.shape[1]:\n",
    "        samples = samples.transpose()\n",
    "        \n",
    "        \n",
    "    augmented_samples = augment(samples=samples, sample_rate=sample_rate)\n",
    "    if len(augmented_samples.shape) == 2:\n",
    "        augmented_samples = augmented_samples.transpose()\n",
    "\n",
    "    for aug_idx in range(NUM_AUGMENTS):        \n",
    "        output_file_path = os.path.join(output_dir_aug, f\"{wav_file}_aug_{aug_idx}.wav\")\n",
    "        wavfile.write(output_file_path, rate=sample_rate, data=augmented_samples)        \n",
    "                \n",
    "    #display(Audio(filename=file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original wav files to the augmented folder\n",
    "import shutil, glob\n",
    "[shutil.copy2(f, output_dir_aug) for f in glob.glob(f\"{output_dir}/*\") if os.path.isfile(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Zip file for custom model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   For audio + human - labeled data(Acoustic type) to train a custom speech model, you need to make a zip file of the audio files and the corresponding text file.\n",
    "-   Here is an example of the structure of the labeled text file.\n",
    "\n",
    "    > ```text\n",
    "    > audio1.wav\tContent like data, models, tests, and endpoints are organized into Projects in the Custom Speech portal. Each project is specific to a domain and country/language. For example, you may create a project for call centers that use English in the United States. To create your first project, select the Speech-to-text/Custom speech, then click New Project. Follow the instructions provided by the wizard to create your project. After you've created a project, you should see four tabs: Data, Testing, Training, and Deployment. Use the links provided in Next steps to learn how to use each tab.\n",
    "    > audio2.wav\tCustom Speech provides tools that allow you to visually inspect the recognition quality of a model by comparing audio data with the corresponding recognition result. From the Custom Speech portal, you can play back uploaded audio and determine if the provided recognition result is correct. This tool allows you to quickly inspect quality of Microsoft's baseline speech-to-text model or a trained custom model without having to transcribe any audio data.\n",
    "    > ```\n",
    "\n",
    "    ```\n",
    "\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "DELETE_OLD_DATA = True\n",
    "USE_AUGMENTED_DATA = True\n",
    "if USE_AUGMENTED_DATA:\n",
    "    output_dir = output_dir_aug\n",
    "\n",
    "train_dataset_dir = \"train_dataset\"\n",
    "if not os.path.exists(train_dataset_dir):\n",
    "    os.makedirs(train_dataset_dir)\n",
    "\n",
    "if(DELETE_OLD_DATA):\n",
    "    for file in os.listdir(train_dataset_dir):\n",
    "        os.remove(os.path.join(train_dataset_dir, file))    \n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "zip_filename = f'train_{language}_{timestamp}.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files:\n",
    "        zipf.write(os.path.join(output_dir, file), file)\n",
    "\n",
    "print(f\"Created zip file: {zip_filename}\")\n",
    "\n",
    "shutil.move(zip_filename, os.path.join(train_dataset_dir, zip_filename))\n",
    "print(f\"Moved zip file to: {os.path.join(train_dataset_dir, zip_filename)}\")\n",
    "print(f\"Moved zip file to: {os.path.join(train_dataset_dir, zip_filename)}\")\n",
    "train_dataset_path = {os.path.join(train_dataset_dir, zip_filename)}\n",
    "%store train_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Evaluation dataset for custom model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Check https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt for supported locale\n",
    "language = CUSTOM_SPEECH_LOCALE\n",
    "\n",
    "#output_dir = \"synthetic_data\"\n",
    "output_dir = \"synthetic_data_daekeun\"\n",
    "DELETE_OLD_DATA = True\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if(DELETE_OLD_DATA):\n",
    "    for file in os.listdir(output_dir):\n",
    "        os.remove(os.path.join(output_dir, file))    \n",
    "\n",
    "train_tts_voices = TTS_FOR_TRAIN.split(',')\n",
    "\n",
    "for tts_voice in train_tts_voices:\n",
    "    with open(synthetic_text_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                expression = json.loads(line)\n",
    "                no = expression['no']\n",
    "                text = expression[language]\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                file_name = f\"{no}_locale_{language}_speaker_{tts_voice}_{timestamp}.wav\"\n",
    "                print(f\"Generating {file_name}\")\n",
    "                get_audio_file_by_speech_synthesis(text, os.path.join(output_dir,file_name), language, tts_voice)\n",
    "                with open(f'{output_dir}/manifest.txt', 'a', encoding='utf-8') as manifest_file:\n",
    "                    manifest_file.write(f\"{file_name}\\t{text}\\n\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(TTS_FOR_EVAL) \n",
    "\n",
    "eval_output_dir = \"synthetic_eval_data\"\n",
    "DELETE_OLD_DATA = True\n",
    "\n",
    "if not os.path.exists(eval_output_dir):\n",
    "    os.makedirs(eval_output_dir)\n",
    "\n",
    "if(DELETE_OLD_DATA):\n",
    "    for file in os.listdir(eval_output_dir):\n",
    "        os.remove(os.path.join(eval_output_dir, file))\n",
    "\n",
    "eval_tts_voices = TTS_FOR_EVAL.split(',')\n",
    "\n",
    "for tts_voice in eval_tts_voices:\n",
    "    with open(synthetic_text_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                expression = json.loads(line)\n",
    "                no = expression['no']\n",
    "                text = expression[language]\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                file_name = f\"{no}_{language}_{timestamp}.wav\"\n",
    "                get_audio_file_by_speech_synthesis(text, os.path.join(eval_output_dir,file_name), language, tts_voice)\n",
    "                with open(f'{eval_output_dir}/manifest.txt', 'a', encoding='utf-8') as manifest_file:\n",
    "                    manifest_file.write(f\"{file_name}\\t{text}\\n\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a zip file for the evaluation dataset and move it to the eval_dataset directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "DELETE_OLD_DATA = True\n",
    "\n",
    "eval_dataset_dir = \"eval_dataset\"\n",
    "if not os.path.exists(eval_dataset_dir):\n",
    "    os.makedirs(eval_dataset_dir)\n",
    "\n",
    "if(DELETE_OLD_DATA):\n",
    "    for file in os.listdir(eval_dataset_dir):\n",
    "        os.remove(os.path.join(eval_dataset_dir, file))    \n",
    "\n",
    "files = os.listdir(eval_output_dir)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "zip_filename = f'eval_{language}_{timestamp}.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files:\n",
    "        zipf.write(os.path.join(eval_output_dir, file), file)\n",
    "\n",
    "print(f\"Created zip file: {zip_filename}\")\n",
    "\n",
    "\n",
    "shutil.move(zip_filename, os.path.join(eval_dataset_dir, zip_filename))\n",
    "print(f\"Moved zip file to: {os.path.join(eval_dataset_dir, zip_filename)}\")\n",
    "eval_dataset_path = {os.path.join(eval_dataset_dir, zip_filename)}\n",
    "%store eval_dataset_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
