{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Speech to Text] Create Custom Speech Model for Italian Language\n",
    "This sample demonstrates how to create Custom Speech model calling REST API. \n",
    "\n",
    "> ✨ ***Note*** <br>\n",
    "> Please check the custom speech support for each language before you get started - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt#:~:text=Custom%20speech%20support \n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## 1. Test STT(Speech to Text) of Azure AI Speech by the synthetic dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset/train_it-IT_20241203083636.zip'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from utils.common import *\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "speech_key = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "speech_region = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "\n",
    "train_dataset_path = \"\"\n",
    "%store -r train_dataset_path\n",
    "try:\n",
    "    train_dataset_path\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the previous notebook again.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "train_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Base URL for the Speech Services REST API\n",
    "base_url = f'https://{speech_region}.api.cognitive.microsoft.com/speechtotext'\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': speech_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_recognition_from_file(file_path: str, lang:str):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region, speech_recognition_language=lang)\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_path)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    return speech_recognition_result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the sorted wav files from the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_it-IT_20241203083554.wav',\n",
       " '2_it-IT_20241203083555.wav',\n",
       " '3_it-IT_20241203083557.wav',\n",
       " '4_it-IT_20241203083558.wav',\n",
       " '5_it-IT_20241203083559.wav',\n",
       " '6_it-IT_20241203083600.wav',\n",
       " '7_it-IT_20241203083601.wav',\n",
       " '8_it-IT_20241203083602.wav',\n",
       " '9_it-IT_20241203083603.wav',\n",
       " '10_it-IT_20241203083604.wav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "output_folder = 'synthetic_data'\n",
    "files = os.listdir(output_folder)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come posso attivare il servizio di assistenza clienti LG?\n",
      "Qual è il numero di telefono per il servizio clienti LG?\n",
      "Ho bisogno di assistenza per il mio televisore LG, chi posso contattare?\n"
     ]
    }
   ],
   "source": [
    "for wav_file in wav_files[0:3]:\n",
    "    print(speech_recognition_from_file(os.path.join(output_folder,wav_file), \"it-IT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload training datasets \n",
    "- You can upload datasets for training, qualitative inspection, and quantitative measurement. \n",
    "- This lab covers two types (Acoustic and Plain text) of training and testing data that you can use for custom speech. \n",
    "- Check the other options on this link - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-custom-speech-test-and-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project created with ID: d5c175ce-1550-45dd-8132-c1c18fd5d3e2\n"
     ]
    }
   ],
   "source": [
    "display_name = \"My Custom Model Training And Evaluation Project\"\n",
    "description = \"Project for training and evaluating the Italian base model\"\n",
    "locale = \"it-IT\"\n",
    "project_id = create_project(base_url, headers, display_name, description, locale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'project_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# Store the project_id for later use\n",
    "%store project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the acoustic dataset to storage (zip files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded successfully.\n",
      "uploaded_files: ['train_it-IT_20241203083636']\n",
      "url: {'train_it-IT_20241203083636': 'https://aoaihub1storageaccount.blob.core.windows.net/stt-container/train_it-IT_20241203083636.zip?se=2024-12-03T17%3A18%3A13Z&sp=r&sv=2025-01-05&sr=b&sig=%2BIkaXF/4CkDa2hZSWJRqQy3TuEiSywp7eMOXG%2B0oRxw%3D'}\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"train_dataset\"\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "account_key = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "uploaded_files, url = upload_dataset_to_storage(data_folder, container_name, account_name, account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets with the uploaded acoustic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with ID: 683c1a42-1530-481a-843b-d897093585d7\n"
     ]
    }
   ],
   "source": [
    "kind=\"Acoustic\"\n",
    "display_name = \"acoustic dataset(zip) for training\"\n",
    "description = \"Dataset for training the Italian base model\"\n",
    "\n",
    "zip_dataset_dict = {}\n",
    "\n",
    "for display_name in uploaded_files:\n",
    "    zip_dataset_dict[display_name] = create_dataset(base_url, headers, project_id, url[display_name], kind, display_name, description, locale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the plain text dataset to storage (text files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded successfully.\n",
      "uploaded_files: ['it-IT_20241203083540']\n",
      "url: {'it-IT_20241203083540': 'https://aoaihub1storageaccount.blob.core.windows.net/stt-container/it-IT_20241203083540.txt?se=2024-12-03T17%3A18%3A16Z&sp=r&sv=2025-01-05&sr=b&sig=HOsWjTC5Ggsuup38t8YKogDbI5kZ9zyfVrU7aE0dFPI%3D'}\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"plain_text\"\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "account_key = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "uploaded_files, url = upload_dataset_to_storage(data_folder, container_name, account_name, account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets with the uploaded plain text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with ID: 5408be63-be37-4b33-b19d-49546b6ae4c1\n"
     ]
    }
   ],
   "source": [
    "kind=\"Language\"\n",
    "display_name = \"plain text dataset for training\"\n",
    "description = \"Dataset for training the Italian base model\"\n",
    "\n",
    "plain_dataset_dict = {}\n",
    "\n",
    "for display_name in uploaded_files:\n",
    "    plain_dataset_dict[display_name] = create_dataset(base_url, headers, project_id, url[display_name], kind, display_name, description, locale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Custom Speech Models with the uploaded datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ✨ ***Note*** <br>\n",
    "> Please check which version of base model support for adaptation from baseline model information <br>\n",
    "> for example, Italian language model 20230111, 2e5e70f1-960b-4509-a7c5-102b29227c0b supports 'Language', 'LanguageMarkdown', 'Pronunciation', 'OutputFormatting' adaptation.<br>\n",
    "> check the supportsAdaptationsWith feature of the base_model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models/base/2e5e70f1-960b-4509-a7c5-102b29227c0b',\n",
       " 'links': {'manifest': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models/base/2e5e70f1-960b-4509-a7c5-102b29227c0b/manifest'},\n",
       " 'properties': {'deprecationDates': {'adaptationDateTime': '2025-01-15T00:00:00Z',\n",
       "   'transcriptionDateTime': '2025-04-15T00:00:00Z'},\n",
       "  'features': {'supportsTranscriptions': True,\n",
       "   'supportsEndpoints': True,\n",
       "   'supportsTranscriptionsOnSpeechContainers': False,\n",
       "   'supportsAdaptationsWith': ['Language',\n",
       "    'LanguageMarkdown',\n",
       "    'Pronunciation',\n",
       "    'OutputFormatting'],\n",
       "   'supportedOutputFormats': ['Display', 'Lexical']},\n",
       "  'chargeForAdaptation': False},\n",
       " 'lastActionDateTime': '2023-02-06T10:32:49Z',\n",
       " 'status': 'Succeeded',\n",
       " 'createdDateTime': '2023-02-06T10:01:46Z',\n",
       " 'locale': 'it-IT',\n",
       " 'displayName': '20230111',\n",
       " 'description': 'it-IT base model'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the model id from the train a new model (UI) in the Azure Speech Studio. \n",
    "# The base model ids are vary from each language \n",
    "base_model_id = \"2e5e70f1-960b-4509-a7c5-102b29227c0b\"  # Italian base model id d822ba77-3a5b-460d-a19c-fd1919026148\n",
    "base_model = get_base_model(base_url, headers, base_model_id)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the custom speech model with plain text datasets (txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'displayName': 'it_custom_model_with_plain_text', 'description': 'Custom model training with plain text dataset', 'locale': 'it-IT', 'baseModel': {'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models/base/d822ba77-3a5b-460d-a19c-fd1919026148'}, 'datasets': [{'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/datasets/5408be63-be37-4b33-b19d-49546b6ae4c1'}], 'project': {'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/projects/d5c175ce-1550-45dd-8132-c1c18fd5d3e2'}}\n",
      "HTTP error occurred: 400 Client Error: Bad Request for url: https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models\n",
      "Response content: b'{\\n  \"code\": \"InvalidRequest\",\\n  \"message\": \"The selected end-to-end model isn\\'t valid for adaptation\",\\n  \"innerError\": {\\n    \"code\": \"InvalidPayload\",\\n    \"message\": \"The selected end-to-end model isn\\'t valid for adaptation\"\\n  }\\n}'\n"
     ]
    }
   ],
   "source": [
    "display_name = \"it_custom_model_with_plain_text\"\n",
    "description = \"Custom model training with plain text dataset\"\n",
    "try:\n",
    "\tcustom_model_with_plain_id = create_custom_model(base_url, headers, project_id, base_model_id, list(plain_dataset_dict.values()), display_name, description, locale)\n",
    "except requests.exceptions.HTTPError as e:\n",
    "\tprint(f\"HTTP error occurred: {e}\")\n",
    "\tprint(f\"Response content: {e.response.content}\")\n",
    "\traise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the custom speech model with acoustic datasets (zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'displayName': 'it_custom_model_with_aocustic_dataset', 'description': 'Custom model training with acoustic dataset', 'locale': 'it-IT', 'baseModel': {'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models/base/d822ba77-3a5b-460d-a19c-fd1919026148'}, 'datasets': [{'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/datasets/f1f13383-f97e-480e-b603-d5b16f6ea6ac'}], 'project': {'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/projects/2d47dabb-6c19-4f57-bab3-85aeb806408b'}}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m display_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit_custom_model_with_aocustic_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom model training with acoustic dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m custom_model_with_acoustic_id \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_custom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mzip_dataset_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocale\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 26\u001b[0m, in \u001b[0;36mcreate_custom_model\u001b[0;34m(base_url, headers, project_id, base_model_id, datasets, display_name, description, locale)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(custom_model_body)\n\u001b[1;32m     25\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(custom_model_url, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mcustom_model_body)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     28\u001b[0m custom_model_id \u001b[38;5;241m=\u001b[39m custom_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/afh/code/Azure_OpenAI_samples/.venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models"
     ]
    }
   ],
   "source": [
    "display_name = \"it_custom_model_with_aocustic_dataset\"\n",
    "description = \"Custom model training with acoustic dataset\"\n",
    "try:\n",
    "\tcustom_model_with_acoustic_id = create_custom_model(base_url, headers, project_id, base_model_id, list(zip_dataset_dict.values()), display_name, description, locale)\n",
    "except requests.exceptions.HTTPError as e:\n",
    "\tprint(f\"HTTP error occurred: {e}\")\n",
    "\tprint(f\"Response content: {e.response.content}\")\n",
    "\traise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_training_status(custom_model_id):\n",
    "    with tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = get_custom_model_status(base_url, headers, custom_model_id)\n",
    "        if status == \"NotStarted\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Succeeded\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = get_custom_model_status(base_url, headers, custom_model_id)\n",
    "        while(pbar.n < 3):\n",
    "            pbar.update(1)\n",
    "        print(\"Training Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitor training status for each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status:   0%|          | 0/3 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status:  67%|██████▋   | 2/3 [00:10<00:05,  5.04s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status: 100%|██████████| 3/3 [02:10<00:00, 43.53s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status: 100%|██████████| 3/3 [00:00<00:00, 100.35step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monitor_training_status(custom_model_with_plain_id)\n",
    "monitor_training_status(custom_model_with_acoustic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'custom_model_with_plain_id' (str)\n",
      "Stored 'custom_model_with_acoustic_id' (str)\n"
     ]
    }
   ],
   "source": [
    "%store custom_model_with_plain_id\n",
    "%store custom_model_with_acoustic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
