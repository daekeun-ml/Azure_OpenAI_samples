{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Speech to Text] Create Custom Speech Model\n",
    "\n",
    "This sample demonstrates how to create Custom Speech model calling REST API.\n",
    "\n",
    "> ✨ **_Note_** <br>\n",
    "> Please check the custom speech support for each language before you get started - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt#:~:text=Custom%20speech%20support\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Configure a Python virtual environment for 3.10 or later:\n",
    "\n",
    "1.  open the Command Palette (Ctrl+Shift+P).\n",
    "1.  Search for Python: Create Environment.\n",
    "1.  select Venv / Conda and choose where to create the new environment.\n",
    "1.  Select the Python interpreter version. Create with version 3.10 or later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## 1. Check the synthetic dataset created by TTS in Azure AI Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from utils.common import *\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SPEECH_KEY = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "SPEECH_REGION = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "CUSTOM_SPEECH_LANG = os.getenv(\"CUSTOM_SPEECH_LANG\")\n",
    "CUSTOM_SPEECH_LOCALE = os.getenv(\"CUSTOM_SPEECH_LOCALE\")\n",
    "TTS_FOR_TRAIN = os.getenv(\"TTS_FOR_TRAIN\")\n",
    "TTS_FOR_EVAL = os.getenv(\"TTS_FOR_eval\")\n",
    "\n",
    "train_dataset_path = \"\"\n",
    "%store -r train_dataset_path\n",
    "eval_dataset_path = \"\"\n",
    "%store -r eval_dataset_path\n",
    "try:\n",
    "    train_dataset_path\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the previous notebook again.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "train_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Base URL for the Speech Services REST API\n",
    "base_url = f'https://{SPEECH_REGION}.api.cognitive.microsoft.com/speechtotext'\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': SPEECH_KEY,\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_recognition_from_file(file_path: str, lang:str):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION, speech_recognition_language=lang)\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_path)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    return speech_recognition_result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the sorted wav files from the dataset folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "output_folder = 'synthetic_data'\n",
    "files = os.listdir(output_folder)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wav_file in wav_files[0:3]:\n",
    "    print(speech_recognition_from_file(os.path.join(output_folder,wav_file), CUSTOM_SPEECH_LOCALE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload training datasets\n",
    "\n",
    "-   You can upload datasets for training, qualitative inspection, and quantitative measurement.\n",
    "-   This lab covers two types (Acoustic and Plain text) of training and testing data that you can use for custom speech.\n",
    "-   Check the other options on this link - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-custom-speech-test-and-train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "utc = pytz.timezone('UTC')\n",
    "date = datetime.now(utc).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "display_name = f\"[{CUSTOM_SPEECH_LANG}] My Custom Speech Project ({date})[UTC]\"\n",
    "description = f\"Project for training and evaluating the {CUSTOM_SPEECH_LANG} base model\"\n",
    "project_id = create_project(base_url, headers, display_name, description, CUSTOM_SPEECH_LOCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the project_id for later use\n",
    "%store project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the acoustic dataset to storage (zip files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"train_dataset\"\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "account_key = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "uploaded_files, url = upload_dataset_to_storage(data_folder, container_name, account_name, account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets with the uploaded acoustic dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind=\"Acoustic\"\n",
    "display_name = \"acoustic dataset(zip) for training\"\n",
    "description = f\"[training] Dataset for fine-tuning the {CUSTOM_SPEECH_LANG} base model\"\n",
    "\n",
    "zip_dataset_dict = {}\n",
    "\n",
    "for display_name in uploaded_files:\n",
    "    zip_dataset_dict[display_name] = create_dataset(base_url, headers, project_id, url[display_name], kind, display_name, description, CUSTOM_SPEECH_LOCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the plain text dataset to storage (text files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"plain_text\"\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "account_key = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "uploaded_files, url = upload_dataset_to_storage(data_folder, container_name, account_name, account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets with the uploaded plain text dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind=\"Language\"\n",
    "display_name = \"plain text dataset for training\"\n",
    "description = f\"[training] Dataset for fine-tuning the {CUSTOM_SPEECH_LANG} base model\"\n",
    "\n",
    "plain_dataset_dict = {}\n",
    "\n",
    "for display_name in uploaded_files:\n",
    "    plain_dataset_dict[display_name] = create_dataset(base_url, headers, project_id, url[display_name], kind, display_name, description, CUSTOM_SPEECH_LOCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Custom Speech Models with the uploaded datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ✨ **_Note_** <br>\n",
    "> Please check which version of base model support for adaptation from baseline model information. <br>\n",
    "> For example, Italian language model 20230111 supports 'Language', 'LanguageMarkdown', 'Pronunciation', 'OutputFormatting' adaptation.<br>\n",
    "> check the supports 'Language' Adaptations With feature of the base_model object.<br>\n",
    "> If you don't specify the baseModel, the default base model for the locale is used.<br>\n",
    "> The base model ids are vary from each language <br>\n",
    "> check the model id from the train a new model (UI) in the Azure Speech Studio if you want to select a base sepecific model. <br>\n",
    "\n",
    "-   Italian 2e5e70f1-960b-4509-a7c5-102b29227c0b\n",
    "-   Vietnamese 8066b5fb-0114-4837-90b6-0c245928a896\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#option1. check the model id from the train a new model (UI) in the Azure Speech Studio. \n",
    "base_model_id = \"8066b5fb-0114-4837-90b6-0c245928a896\"  # Vietnamese base model id\n",
    "\n",
    "#option2. check the model id from the API call\n",
    "base_model = get_latest_base_model(base_url, headers, f\"locale eq '{CUSTOM_SPEECH_LOCALE}' and status eq 'Succeeded'\")\n",
    "# Filter the base models to find the ones that support 'Language' adaptations and have the latest lastActionDateTime\n",
    "filtered_models = [model for model in base_model['values'] if 'properties' in model  and 'Language' in model['properties']['features'].get('supportsAdaptationsWith', [])]\n",
    "if filtered_models:\n",
    "\tlatest_model = max(filtered_models, key=lambda x: x['createdDateTime'])\n",
    "\tprint(\"Latest model supporting 'Language' adaptations:\")\n",
    "else:\n",
    "\tprint(\"No models found that support 'Language' adaptations.\")\n",
    "print(latest_model)\n",
    "# Check if you are charged for training this model. \n",
    "# Here is the reference document of the Charge for adaptation: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/migrate-v3-1-to-v3-2#charge-for-adaptation\n",
    "print(\"Charge for Adaptation:\", latest_model['properties']['chargeForAdaptation'])\n",
    "# Get the latest model ID from the self link for example 8066b5fb-0114-4837-90b6-0c245928a896 is the model id in 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models/base/8066b5fb-0114-4837-90b6-0c245928a896' \n",
    "base_model_id = latest_model['self'].split('/')[-1]\n",
    "print(base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the custom speech model with plain text datasets (txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = f\"[{CUSTOM_SPEECH_LOCALE}] custom_model_with_plain_text\"\n",
    "description = f\"{CUSTOM_SPEECH_LANG} Custom model training with plain text dataset\"\n",
    "custom_model_with_plain_id = create_custom_model(base_url, headers, project_id, base_model_id, list(plain_dataset_dict.values()), display_name, description, CUSTOM_SPEECH_LOCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the custom speech model with acoustic datasets (zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = f\"[{CUSTOM_SPEECH_LOCALE}] custom_model_with_aocustic_dataset\"\n",
    "description = f\"{CUSTOM_SPEECH_LANG} Custom model training with acoustic dataset\"\n",
    "custom_model_with_acoustic_id = create_custom_model(base_url, headers, project_id, base_model_id, list(zip_dataset_dict.values()), display_name, description, CUSTOM_SPEECH_LOCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_training_status(custom_model_id):\n",
    "    with tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = get_custom_model_status(base_url, headers, custom_model_id)\n",
    "        if status == \"NotStarted\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Succeeded\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = get_custom_model_status(base_url, headers, custom_model_id)\n",
    "        while(pbar.n < 3):\n",
    "            pbar.update(1)\n",
    "        print(\"Training Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitor training status for each job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_training_status(custom_model_with_plain_id)\n",
    "monitor_training_status(custom_model_with_acoustic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store custom_model_with_plain_id\n",
    "%store custom_model_with_acoustic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
