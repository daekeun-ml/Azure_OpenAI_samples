{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Text To Speech] Synthetic Audio Dataset Creation\n",
    "This sample demonstrates how to use Azure AI Speech API to generate synthetic audio dataset from text. \n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> Please check the supported languages and region availabilty before you get started - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts / https://learn.microsoft.com/en-us/azure/ai-services/speech-service/regions  \n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Speech SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "import html\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "SPEECH_KEY = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "SPEECH_REGION = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "CUSTOM_SPEECH_LANG = os.getenv(\"CUSTOM_SPEECH_LANG\")\n",
    "CUSTOM_SPEECH_LOCALE = os.getenv(\"CUSTOM_SPEECH_LOCALE\")\n",
    "SPEECH_KEY = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "SPEECH_REGION = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "CUSTOM_SPEECH_LANG = os.getenv(\"CUSTOM_SPEECH_LANG\")\n",
    "CUSTOM_SPEECH_LOCALE = os.getenv(\"CUSTOM_SPEECH_LOCALE\")\n",
    "TTS_FOR_QUESTION = os.getenv(\"TTS_FOR_QUESTION\")\n",
    "TTS_FOR_ANSWER = os.getenv(\"TTS_FOR_ANSWER\")\n",
    "\n",
    "\n",
    "phi_api_endpoint = os.getenv(\"AZURE_PHI3.5_ENDPOINT\")\n",
    "phi_api_key = os.getenv(\"AZURE_PHI3.5_API_KEY\")\n",
    "phi_deployment_name = os.getenv(\"AZURE_PHI3.5_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    client = ChatCompletionsClient(\n",
    "    #endpoint=\"https://aoai-services1.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview\", # you will run into a 500 error if you use this endpoint\n",
    "    endpoint=\"https://aoai-services1.services.ai.azure.com/models\",\n",
    "    credential=AzureKeyCredential(phi_api_key),\n",
    ")\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of a speech config with specified subscription key and service region.\n",
    "Replace with your own subscription key and service region (e.g., \"westus\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Question and Answer Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "topic = f\"\"\"\n",
    "Contoso Electronics call center question and answer related expected spoken utterances for {CUSTOM_SPEECH_LANG} and English languages.\n",
    "\"\"\"\n",
    "question = f\"\"\"\n",
    "create 10 lines of jsonl of the topic in {CUSTOM_SPEECH_LANG} and english. jsonl format is required. use 'no' as number 'type' including 'question' or 'answer' and '{CUSTOM_SPEECH_LOCALE}', 'en-US' keys for the languages.\n",
    "only include the lines as the result. Do not include ```jsonl, ``` and blank line in the result. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"\"\"\n",
    "        Generate plain text sentences of #topic# related text to improve the recognition of domain-specific words and phrases.\n",
    "        Domain-specific words can be uncommon or made-up words, but their pronunciation must be straightforward to be recognized. \n",
    "        Use text data that's close to the expected spoken utterances. The nummber of utterances per line should be 1. \n",
    "        Here is examples of the expected format:\n",
    "        {\"no\": 1, \"string\": \"string\", \"string\": \"string\", \"string\": \"string\"}\n",
    "        {\"no\": 2, \"string\": \"string\", \"string\": \"string\", \"string\": \"string\"} \n",
    "        \"\"\"),\n",
    "\n",
    "       \n",
    "        UserMessage(content=f\"\"\"\n",
    "        #topic#: {topic}\n",
    "        Question: {question}\n",
    "        \"\"\"),\n",
    "    ],\n",
    "    # Simply change the model name for the appropiate model \"Phi-3.5-mini-instruct\" or \"Phi-3.5-vision-instruct\"\n",
    "    model=phi_deployment_name, \n",
    "    temperature=0.8,\n",
    "    max_tokens=2048,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "print(content)\n",
    "print(\"Usage Information:\")\n",
    "#print(f\"Cached Tokens: {response.usage.prompt_tokens_details.cached_tokens}\") #only o1 models support this\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Speaker Identification Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "languages = [CUSTOM_SPEECH_LOCALE]  # List of languages to generate audio files\n",
    "output_dir = \"synthetic_two_speaker_data\"\n",
    "DELETE_OLD_DATA = True\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if DELETE_OLD_DATA:\n",
    "    for file in os.listdir(output_dir):\n",
    "        os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "for i in range(0, len(content.strip().split('\\n')), 2):\n",
    "    try:\n",
    "        question_line = content.strip().split('\\n')[i]\n",
    "        answer_line = content.strip().split('\\n')[i + 1]\n",
    "        \n",
    "        question_expression = json.loads(question_line)\n",
    "        answer_expression = json.loads(answer_line)\n",
    "        \n",
    "        question_no = question_expression['no']\n",
    "        question_text = question_expression[CUSTOM_SPEECH_LOCALE]\n",
    "        question_tts_voice = TTS_FOR_QUESTION\n",
    "        \n",
    "        answer_text = answer_expression[CUSTOM_SPEECH_LOCALE]\n",
    "        answer_tts_voice = TTS_FOR_ANSWER\n",
    "        \n",
    "        combined_text = f\"{question_text} {answer_text}\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        file_name = f\"{question_no}_{CUSTOM_SPEECH_LOCALE}_{timestamp}.wav\"\n",
    "        \n",
    "        ssml = f\"\"\"<speak version='1.0'  xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang='{CUSTOM_SPEECH_LOCALE}'>\n",
    "                        <voice name='{question_tts_voice}'>\n",
    "                                {html.escape(question_text)}\n",
    "                        </voice>\n",
    "                        <voice name='{answer_tts_voice}'>\n",
    "                                {html.escape(answer_text)}\n",
    "                        </voice>\n",
    "                    </speak>\"\"\"\n",
    "        \n",
    "        speech_sythesis_result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "        stream = speechsdk.AudioDataStream(speech_sythesis_result)\n",
    "        stream.save_to_wav_file(os.path.join(output_dir, file_name))\n",
    "        \n",
    "        with open(f'{output_dir}/manifest.txt', 'a', encoding='utf-8') as manifest_file:\n",
    "            manifest_file.write(f\"{file_name}\\t{combined_text}\\n\")\n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        print(f\"Error processing lines {i} and {i + 1}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the speaker diarization with the generated audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "files = os.listdir(output_dir)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play each WAV file in the output folder\n",
    "for wav_file in wav_files[:2]:\n",
    "    file_path = os.path.join(output_dir, wav_file)\n",
    "    display(Audio(filename=file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_transcriber_recognition_canceled_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('Canceled event')\n",
    "\n",
    "def conversation_transcriber_session_stopped_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('SessionStopped event')\n",
    "\n",
    "def conversation_transcriber_transcribed_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    print('\\nTRANSCRIBED:')\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print('\\tText={}'.format(evt.result.text))\n",
    "        print('\\tSpeaker ID={}\\n'.format(evt.result.speaker_id))\n",
    "    elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print('\\tNOMATCH: Speech could not be TRANSCRIBED: {}'.format(evt.result.no_match_details))\n",
    "\n",
    "def conversation_transcriber_transcribing_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    print('TRANSCRIBING:')\n",
    "    print('\\tText={}'.format(evt.result.text))\n",
    "    print('\\tSpeaker ID={}'.format(evt.result.speaker_id))\n",
    "\n",
    "def conversation_transcriber_session_started_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('SessionStarted event')\n",
    "\n",
    "def speech_recognition_from_file(file_path: str, lang:str):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION, speech_recognition_language=lang)\n",
    "    speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults, value='true')\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_path)\n",
    "    conversation_transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    transcribing_stop = False\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        #\"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal transcribing_stop\n",
    "        transcribing_stop = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the conversation transcriber\n",
    "    conversation_transcriber.transcribed.connect(conversation_transcriber_transcribed_cb)\n",
    "    conversation_transcriber.transcribing.connect(conversation_transcriber_transcribing_cb)\n",
    "    conversation_transcriber.session_started.connect(conversation_transcriber_session_started_cb)\n",
    "    conversation_transcriber.session_stopped.connect(conversation_transcriber_session_stopped_cb)\n",
    "    conversation_transcriber.canceled.connect(conversation_transcriber_recognition_canceled_cb)\n",
    "    # stop transcribing on either session stopped or canceled events\n",
    "    conversation_transcriber.session_stopped.connect(stop_cb)\n",
    "    conversation_transcriber.canceled.connect(stop_cb)\n",
    "\n",
    "    conversation_transcriber.start_transcribing_async()\n",
    "\n",
    "    # Waits for completion.\n",
    "    while not transcribing_stop:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    conversation_transcriber.stop_transcribing_async()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wav_file in wav_files[0:3]:\n",
    "    speech_recognition_from_file(os.path.join(output_dir, wav_file), CUSTOM_SPEECH_LOCALE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
