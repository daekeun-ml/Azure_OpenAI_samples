{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Text To Speech] Synthetic Audio Dataset Creation\n",
    "\n",
    "This sample demonstrates how to use Azure AI Speech API to generate synthetic audio dataset from text.\n",
    "\n",
    "> âœ¨ **_Note_** <br>\n",
    "> Please check the supported languages and region availabilty before you get started - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts / https://learn.microsoft.com/en-us/azure/ai-services/speech-service/regions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Configure a Python virtual environment for 3.10 or later:\n",
    "\n",
    "1.  open the Command Palette (Ctrl+Shift+P).\n",
    "1.  Search for Python: Create Environment.\n",
    "1.  select Venv / Conda and choose where to create the new environment.\n",
    "1.  Select the Python interpreter version. Create with version 3.10 or later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Speech SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "import html\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "USE_AOAI = True\n",
    "\n",
    "SPEECH_KEY = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "SPEECH_REGION = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "CUSTOM_SPEECH_LANG = os.getenv(\"CUSTOM_SPEECH_LANG\")\n",
    "CUSTOM_SPEECH_LOCALE = os.getenv(\"CUSTOM_SPEECH_LOCALE\")\n",
    "TTS_FOR_QUESTION = os.getenv(\"TTS_FOR_QUESTION\")\n",
    "TTS_FOR_ANSWER = os.getenv(\"TTS_FOR_ANSWER\")\n",
    "\n",
    "phi_api_endpoint = os.getenv(\"AZURE_PHI3.5_ENDPOINT\")\n",
    "phi_api_key = os.getenv(\"AZURE_PHI3.5_API_KEY\")\n",
    "phi_deployment_name = os.getenv(\"AZURE_PHI3.5_DEPLOYMENT_NAME\")\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "if \"/models\" in phi_api_endpoint:\n",
    "    phi_api_endpoint = phi_api_endpoint.split(\"/models\")[0] + \"/models\"\n",
    "    \n",
    "if \"/models\" in aoai_api_endpoint:\n",
    "    aoai_api_endpoint = aoai_api_endpoint.split(\"/models\")[0] + \"/models\"  \n",
    "    \n",
    "print(\"=== Azure AI Speech Info ===\")\n",
    "print(f\"SPEECH_REGION={SPEECH_REGION}\")\n",
    "print(f\"CUSTOM_SPEECH_LANG={CUSTOM_SPEECH_LANG}\")\n",
    "print(f\"CUSTOM_SPEECH_LOCALE={CUSTOM_SPEECH_LOCALE}\")  \n",
    "print(f\"TTS_FOR_QUESTION={TTS_FOR_QUESTION}\") \n",
    "print(f\"TTS_FOR_ANSWER={TTS_FOR_ANSWER}\\n\") \n",
    "        \n",
    "try:\n",
    "    if USE_AOAI:\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint = aoai_api_endpoint,\n",
    "            api_key        = aoai_api_key,\n",
    "            api_version    = aoai_api_version,\n",
    "        )\n",
    "\n",
    "        print(\"=== Initialized AzuureOpenAI client ===\")\n",
    "        print(f\"AZURE_OPENAI_ENDPOINT={aoai_api_endpoint}\")\n",
    "        print(f\"AZURE_OPENAI_API_VERSION={aoai_api_version}\")\n",
    "        print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "        \n",
    "    else:   \n",
    "        client = ChatCompletionsClient(\n",
    "            #endpoint=\"https://aoai-services1.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview\", # you will run into a 500 error if you use this endpoint\n",
    "            endpoint=phi_api_endpoint,\n",
    "            credential=AzureKeyCredential(phi_api_key)\n",
    "        )\n",
    "        \n",
    "        print(\"=== Initialized AI Inference client ===\")\n",
    "        print(f\"AZURE_PHI3.5_ENDPOINT={phi_api_endpoint}\")\n",
    "        print(f\"AZURE_PHI3.5_API_KEY={phi_api_key}\")\n",
    "        print(f\"AZURE_PHI3.5_DEPLOYMENT_NAME={phi_deployment_name}\")           \n",
    "\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of a speech config with specified subscription key and service region.\n",
    "Replace with your own subscription key and service region (e.g., \"westus\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Question and Answer Text Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "NUM_SAMPLES = 2\n",
    "\n",
    "topic = f\"\"\"\n",
    "Contoso Electronics call center question and answer related expected spoken utterances for {CUSTOM_SPEECH_LANG} and English languages.\n",
    "\"\"\"\n",
    "question = f\"\"\"\n",
    "create {NUM_SAMPLES} lines of jsonl of the topic in {CUSTOM_SPEECH_LANG} and english. jsonl format is required. use 'no' as number 'type' including 'question' or 'answer' and '{CUSTOM_SPEECH_LOCALE}', 'en-US' keys for the languages.\n",
    "only include the lines as the result. Do not include ```jsonl, ``` and blank line in the result. \n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "Generate plain text sentences of #topic# related text to improve the recognition of domain-specific words and phrases.\n",
    "Domain-specific words can be uncommon or made-up words, but their pronunciation must be straightforward to be recognized. \n",
    "Use text data that's close to the expected spoken utterances. The nummber of utterances per line should be 1. \n",
    "Here is examples of the expected format:\n",
    "{\"no\": 1, \"string\": \"string\", \"string\": \"string\", \"string\": \"string\"}\n",
    "{\"no\": 2, \"string\": \"string\", \"string\": \"string\", \"string\": \"string\"} \n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "#topic#: {topic}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "if USE_AOAI:\n",
    "    response = client.chat.completions.create(\n",
    "        model=aoai_deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        temperature=0.8,\n",
    "        max_tokens=1024,\n",
    "        top_p=0.1    \n",
    "    )\n",
    "else:\n",
    "    response = client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(content=system_message),\n",
    "            UserMessage(content=user_message),\n",
    "        ],\n",
    "        # Simply change the model name for the appropiate model \"Phi-3.5-mini-instruct\" or \"Phi-3.5-vision-instruct\"\n",
    "        model=phi_deployment_name, \n",
    "        temperature=0.8,\n",
    "        max_tokens=1024,\n",
    "        top_p=0.1\n",
    "    )    \n",
    "    \n",
    "content = response.choices[0].message.content\n",
    "print(content)\n",
    "print(\"Usage Information:\")\n",
    "#print(f\"Cached Tokens: {response.usage.prompt_tokens_details.cached_tokens}\") #only o1 models support this\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Speaker Identification Audio Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "languages = [CUSTOM_SPEECH_LOCALE]  # List of languages to generate audio files\n",
    "output_dir = \"synthetic_two_speaker_data\"\n",
    "DELETE_OLD_DATA = True\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if DELETE_OLD_DATA:\n",
    "    for file in os.listdir(output_dir):\n",
    "        os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "for i in range(0, len(content.strip().split('\\n')), 2):\n",
    "    try:\n",
    "        question_line = content.strip().split('\\n')[i]\n",
    "        answer_line = content.strip().split('\\n')[i + 1]\n",
    "        \n",
    "        question_expression = json.loads(question_line)\n",
    "        answer_expression = json.loads(answer_line)\n",
    "        \n",
    "        question_no = question_expression['no']\n",
    "        question_text = question_expression[CUSTOM_SPEECH_LOCALE]\n",
    "        question_tts_voice = TTS_FOR_QUESTION\n",
    "        \n",
    "        answer_text = answer_expression[CUSTOM_SPEECH_LOCALE]\n",
    "        answer_tts_voice = TTS_FOR_ANSWER\n",
    "        \n",
    "        combined_text = f\"{question_text} {answer_text}\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        file_name = f\"{question_no}_{CUSTOM_SPEECH_LOCALE}_{timestamp}.wav\"\n",
    "        print(f\"Generating {file_name}\")        \n",
    "        \n",
    "        ssml = f\"\"\"<speak version='1.0'  xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang='{CUSTOM_SPEECH_LOCALE}'>\n",
    "                        <voice name='{question_tts_voice}'>\n",
    "                                {html.escape(question_text)}\n",
    "                        </voice>\n",
    "                        <voice name='{answer_tts_voice}'>\n",
    "                                {html.escape(answer_text)}\n",
    "                        </voice>\n",
    "                    </speak>\"\"\"\n",
    "        \n",
    "        speech_sythesis_result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "        stream = speechsdk.AudioDataStream(speech_sythesis_result)\n",
    "        stream.save_to_wav_file(os.path.join(output_dir, file_name))\n",
    "        \n",
    "        with open(f'{output_dir}/manifest.txt', 'a', encoding='utf-8') as manifest_file:\n",
    "            manifest_file.write(f\"{file_name}\\t{combined_text}\\n\")\n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        print(f\"Error processing lines {i} and {i + 1}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the speaker diarization with the generated audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "files = os.listdir(output_dir)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play each WAV file in the output folder\n",
    "for wav_file in wav_files[:2]:\n",
    "    file_path = os.path.join(output_dir, wav_file)\n",
    "    display(Audio(filename=file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_transcriber_recognition_canceled_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('Canceled event')\n",
    "\n",
    "def conversation_transcriber_session_stopped_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('SessionStopped event')\n",
    "\n",
    "def conversation_transcriber_transcribed_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    print('\\nTRANSCRIBED:')\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print('\\tText={}'.format(evt.result.text))\n",
    "        print('\\tSpeaker ID={}\\n'.format(evt.result.speaker_id))\n",
    "    elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print('\\tNOMATCH: Speech could not be TRANSCRIBED: {}'.format(evt.result.no_match_details))\n",
    "\n",
    "def conversation_transcriber_transcribing_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    print('TRANSCRIBING:')\n",
    "    print('\\tText={}'.format(evt.result.text))\n",
    "    print('\\tSpeaker ID={}'.format(evt.result.speaker_id))\n",
    "\n",
    "def conversation_transcriber_session_started_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('SessionStarted event')\n",
    "\n",
    "def speech_recognition_from_file(file_path: str, lang:str):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION, speech_recognition_language=lang)\n",
    "    speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults, value='true')\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_path)\n",
    "    conversation_transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    transcribing_stop = False\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        #\"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal transcribing_stop\n",
    "        transcribing_stop = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the conversation transcriber\n",
    "    conversation_transcriber.transcribed.connect(conversation_transcriber_transcribed_cb)\n",
    "    conversation_transcriber.transcribing.connect(conversation_transcriber_transcribing_cb)\n",
    "    conversation_transcriber.session_started.connect(conversation_transcriber_session_started_cb)\n",
    "    conversation_transcriber.session_stopped.connect(conversation_transcriber_session_stopped_cb)\n",
    "    conversation_transcriber.canceled.connect(conversation_transcriber_recognition_canceled_cb)\n",
    "    # stop transcribing on either session stopped or canceled events\n",
    "    conversation_transcriber.session_stopped.connect(stop_cb)\n",
    "    conversation_transcriber.canceled.connect(stop_cb)\n",
    "\n",
    "    conversation_transcriber.start_transcribing_async()\n",
    "\n",
    "    # Waits for completion.\n",
    "    while not transcribing_stop:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    conversation_transcriber.stop_transcribing_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wav_file in wav_files[0:3]:\n",
    "    speech_recognition_from_file(os.path.join(output_dir, wav_file), CUSTOM_SPEECH_LOCALE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
