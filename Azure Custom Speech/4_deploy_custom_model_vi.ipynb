{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deploy and Run] Dploy the Trained Custom Speech Models\n",
    "This sample demonstrates how to deploy and run the trained Custom Speech models calling REST API. \n",
    "\n",
    "> ✨ ***Note*** <br>\n",
    "> Except for batch transcription, you must deploy a custom endpoint to use a custom speech model. You can deploy an endpoint for a base or custom model, and then update the endpoint later to use a better trained model. Endpoints used by F0 Speech resources are deleted after seven days.\n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8b0eb25b-38f1-4f7b-8cfe-641753cd9cd1 762281d5-7387-45cf-8a1b-13f2670b15f3 d44c77c4-08b0-4851-aa97-a1417aa3ddc4\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from utils.common import *\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "speech_key = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "speech_region = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "\n",
    "# Get the project and custom model IDs from the previous notebook\n",
    "project_id = \"\"\n",
    "custom_model_with_plain_id = \"\"\n",
    "custom_model_with_acoustic_id = \"\"\n",
    "%store -r project_id\n",
    "%store -r custom_model_with_plain_id\n",
    "%store -r custom_model_with_acoustic_id\n",
    "\n",
    "try:\n",
    "    project_id, custom_model_with_plain_id, custom_model_with_acoustic_id\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the previous notebook again.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "print(project_id, custom_model_with_plain_id, custom_model_with_acoustic_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deploy a trained speech model\n",
    "- In order to get more information about deploying the custom models, check this document link below\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-custom-speech-deploy-model?pivots=rest-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Base URL for the Speech Services REST API\n",
    "base_url = f'https://{speech_region}.api.cognitive.microsoft.com/speechtotext'\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': speech_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the custom speech model ids to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762281d5-7387-45cf-8a1b-13f2670b15f3 d44c77c4-08b0-4851-aa97-a1417aa3ddc4\n"
     ]
    }
   ],
   "source": [
    "# Set the base, custom model id to evaluate\n",
    "base_model_id = \"your base model id\"  ##check the model id from the train a new model ()\n",
    "print(custom_model_with_plain_id, custom_model_with_acoustic_id)\n",
    "\n",
    "# Set the model id to deploy\n",
    "deploy_model_id = custom_model_with_plain_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an endpoint to deploy the custom speech model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/endpoints\n",
      "8b0eb25b-38f1-4f7b-8cfe-641753cd9cd1 762281d5-7387-45cf-8a1b-13f2670b15f3\n",
      "{'model': {'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models/762281d5-7387-45cf-8a1b-13f2670b15f3'}, 'project': {'self': 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/projects/8b0eb25b-38f1-4f7b-8cfe-641753cd9cd1'}, 'displayName': 'My custom model endpoint', 'description': 'Custom model endpoint to deloy', 'locale': 'vi-VN'}\n",
      "Create Endpoint and deploy job finished with ID: 4b91679e-8605-4a30-8496-7b16df2df866\n"
     ]
    }
   ],
   "source": [
    "display_name = \"My custom model endpoint\"\n",
    "description = \"Custom model endpoint to deloy\"\n",
    "locale = \"vi-VN\"\n",
    "\n",
    "endpoint_id = create_endpoint(base_url, headers, project_id, deploy_model_id, display_name, description, locale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_endpoint_status(endpoint_id):\n",
    "    with tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = get_endpoint_status(base_url, headers, endpoint_id)\n",
    "        if status == \"NotStarted\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Succeeded\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = get_endpoint_status(base_url, headers, endpoint_id)\n",
    "        while(pbar.n < 3):\n",
    "            pbar.update(1)\n",
    "        print(\"Endpoint Creation Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status:  33%|███▎      | 1/3 [00:01<00:02,  1.05s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status:  67%|██████▋   | 2/3 [00:12<00:06,  6.93s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Running\n",
      "Current Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status: 100%|██████████| 3/3 [00:34<00:00, 11.41s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Creation Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monitor_endpoint_status(endpoint_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test the deployed custom model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test your deployed custom model by Python SDK. In order to set the endpoint, you need to get the endpoint id from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_recognition_from_file_with_endpoint(file_path: str, lang:str, endoint_id: str):\n",
    "    # Check the Parameters - https://learn.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechconfig?view=azure-python\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region, speech_recognition_language=lang)\n",
    "    speech_config.endpoint_id = endoint_id\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_path)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    return speech_recognition_result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the sorted wav files from the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_vi-VN_20241110220608.wav',\n",
       " '2_vi-VN_20241110220612.wav',\n",
       " '3_vi-VN_20241110220616.wav',\n",
       " '4_vi-VN_20241110220620.wav',\n",
       " '5_vi-VN_20241110220624.wav',\n",
       " '6_vi-VN_20241110220629.wav',\n",
       " '7_vi-VN_20241110220632.wav',\n",
       " '8_vi-VN_20241110220636.wav',\n",
       " '9_vi-VN_20241110220640.wav',\n",
       " '10_vi-VN_20241110220644.wav',\n",
       " '11_vi-VN_20241110220648.wav',\n",
       " '12_vi-VN_20241110220652.wav',\n",
       " '13_vi-VN_20241110220655.wav',\n",
       " '14_vi-VN_20241110220659.wav',\n",
       " '15_vi-VN_20241110220704.wav',\n",
       " '16_vi-VN_20241110220708.wav',\n",
       " '17_vi-VN_20241110220711.wav',\n",
       " '18_vi-VN_20241110220715.wav',\n",
       " '19_vi-VN_20241110220719.wav',\n",
       " '20_vi-VN_20241110220722.wav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "output_folder = 'synthetic_data'\n",
    "files = os.listdir(output_folder)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi có thể được trợ giúp bằng tiếng anh không?\n",
      "Máy giặt của tôi không hoạt động, tôi phải làm gì?\n",
      "Lg có chính sách bảo hành bao lâu?\n"
     ]
    }
   ],
   "source": [
    "for wav_file in wav_files[0:3]:\n",
    "    print(speech_recognition_from_file_with_endpoint(os.path.join(output_folder,wav_file), \"vi-VN\", endpoint_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delete the all resources after testing\n",
    "- Don't forget to delete the deployed custom model and project after testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The endpoint deleted: 4b91679e-8605-4a30-8496-7b16df2df866, None\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint(base_url, headers, endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The project deleted: 8b0eb25b-38f1-4f7b-8cfe-641753cd9cd1, None\n"
     ]
    }
   ],
   "source": [
    "delete_project(base_url, headers, project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
