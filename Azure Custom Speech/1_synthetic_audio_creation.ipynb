{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TTS] Synthetic Audio Dataset Creation\n",
    "This sample demonstrates how to use Azure AI Speech API to generate synthetic audio dataset from text. \n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> Please check the supported languages and region availabilty before you get started - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts / https://learn.microsoft.com/en-us/azure/ai-services/speech-service/regions  \n",
    "\n",
    "## Prerequisites\n",
    "Git clone the repository to your local machine. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/hyogrin/Azure_OpenAI_samples.git\n",
    "```\n",
    "\n",
    "* A subscription key for the Speech service. See [Try the speech service for free](https://docs.microsoft.com/azure/cognitive-services/speech-service/get-started).\n",
    "* Python 3.5 or later needs to be installed. Downloads are available [here](https://www.python.org/downloads/).\n",
    "* The Python Speech SDK package is available for Windows (x64 or x86) and Linux (x64; Ubuntu 16.04 or Ubuntu 18.04).\n",
    "* On Ubuntu 16.04 or 18.04, run the following commands for the installation of required packages:\n",
    "  ```sh\n",
    "  sudo apt-get update\n",
    "  sudo apt-get install libssl1.0.0 libasound2\n",
    "  ```\n",
    "* On Debian 9, run the following commands for the installation of required packages:\n",
    "  ```sh\n",
    "  sudo apt-get update\n",
    "  sudo apt-get install libssl1.0.2 libasound2\n",
    "  ```\n",
    "* On Windows you need the [Microsoft Visual C++ Redistributable for Visual Studio 2017](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) for your platform.\n",
    "\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Speech Synthesis Using the Speech SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "speech_key = os.getenv(\"AZURE_AI_SPEECH_API_KEY\")\n",
    "speech_region = os.getenv(\"AZURE_AI_SPEECH_REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of a speech config with specified subscription key and service region.\n",
    "Replace with your own subscription key and service region (e.g., \"westus\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type some text that you want to speak...\")\n",
    "text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RETRIES = 2\n",
    "for _ in range(MIN_RETRIES):\n",
    "    try:\n",
    "        result = speech_synthesizer.speak_text_async(text).get()\n",
    "    except Exception as e:\n",
    "        time.sleep(10)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"sample\"\n",
    "    \n",
    "\n",
    "if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized to speaker for text [{}]\".format(text))\n",
    "    stream = speechsdk.AudioDataStream(result)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    stream.save_to_wav_file(os.path.join(output_dir, \"result_text.wav\"))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        if cancellation_details.error_details:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    print(\"Did you update the subscription info?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "default_tts_voice = 'en-US-JennyMultilingualV2Neural' # Default TTS voice for English\n",
    "lang = \"en-US\"\n",
    "#default_tts_voice = 'vi-VN-HoaiMyNeural' # Default TTS voice for Vietnamese check the voice gallery for more options\n",
    "#lang = \"vi-VN\"\n",
    "\n",
    "ssml = f\"\"\"<speak version='1.0'  xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang='{lang}'>\n",
    "                     <voice name='{default_tts_voice}'>\n",
    "                             {html.escape(text)}\n",
    "                     </voice>\n",
    "                   </speak>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "speech_sythesis_result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "stream = speechsdk.AudioDataStream(speech_sythesis_result)\n",
    "stream.save_to_wav_file(os.path.join(output_dir,\"result_ssml.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate syntethic dataset and minifest.txt file to train custom speech model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For audio + human - labeled data(Acoustic type) to train a custom speech model, you need to make a zip file of the audio files and the corresponding text file. \n",
    "Here is an example of the structure of the labeled text file.\n",
    "\n",
    "```text\n",
    "audio1.wav\tContent like data, models, tests, and endpoints are organized into Projects in the Custom Speech portal. Each project is specific to a domain and country/language. For example, you may create a project for call centers that use English in the United States. To create your first project, select the Speech-to-text/Custom speech, then click New Project. Follow the instructions provided by the wizard to create your project. After you've created a project, you should see four tabs: Data, Testing, Training, and Deployment. Use the links provided in Next steps to learn how to use each tab.\n",
    "audio2.wav\tCustom Speech provides tools that allow you to visually inspect the recognition quality of a model by comparing audio data with the corresponding recognition result. From the Custom Speech portal, you can play back uploaded audio and determine if the provided recognition result is correct. This tool allows you to quickly inspect quality of Microsoft's baseline speech-to-text model or a trained custom model without having to transcribe any audio data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_audio_file_by_speech_synthesis(text, file_path, lang=\"en-US\", default_tts_voice=\"en-US-JennyMultilingualV2Neural\"):\n",
    "    ssml = f\"\"\"<speak version='1.0'  xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang='{lang}'>\n",
    "                     <voice name='{default_tts_voice}'>\n",
    "                             {html.escape(text)}\n",
    "                     </voice>\n",
    "                   </speak>\"\"\"\n",
    "    speech_sythesis_result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "    stream = speechsdk.AudioDataStream(speech_sythesis_result)\n",
    "    stream.save_to_wav_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic wav dataset and manifest for Vietnamese language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "default_tts_voice = 'vi-VN-HoaiMyNeural' # Default TTS voice for Vietnamese check the voice gallery for more options\n",
    "languages = ['vi-VN'] # List of languages to generate audio files\n",
    "output_dir = \"output\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open('cc_support_expressions.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            expression = json.loads(line)\n",
    "            no = expression['no']\n",
    "            for lang in languages:\n",
    "                text = expression[lang]\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                file_name = f\"{no}_{lang}_{timestamp}.wav\"\n",
    "                get_audio_file_by_speech_synthesis(text, os.path.join(output_dir,file_name), lang, default_tts_voice)\n",
    "                with open('output/manifest.txt', 'a', encoding='utf-8') as manifest_file:\n",
    "                    manifest_file.write(f\"{file_name}\\t{text}\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line}\")\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play WAV Files in Output Folder\n",
    "Use the os library to list all WAV files in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "output_folder = 'output'\n",
    "files = os.listdir(output_folder)\n",
    "wav_files = [file for file in files if file.endswith('.wav')]\n",
    "\n",
    "# Sort wav_files by 'no' in ascending order\n",
    "wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "wav_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play WAV Files\n",
    "Use IPython.display.Audio to play each WAV file listed in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play each WAV file in the output folder\n",
    "for wav_file in wav_files[:3]:\n",
    "    file_path = os.path.join(output_folder, wav_file)\n",
    "    display(Audio(filename=file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "zip_filename = f'train_{lang}_{timestamp}.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files:\n",
    "        zipf.write(os.path.join(output_folder, file), file)\n",
    "\n",
    "print(f\"Created zip file: {zip_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
