{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch evaluation with your own data\n",
    "The following sample shows the basic way to evaluate a Generative AI application in your development environment with the Azure AI evaluation SDK.\n",
    "\n",
    "> ‚ú® ***Note*** <br>\n",
    "> Please check the reference document before you get started - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## üî® Current Support and Limitations (as of 2025-01-14) \n",
    "- Check the region support for the Azure AI Evaluation SDK. https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#region-support\n",
    "\n",
    "### Region support for evaluations\n",
    "| Region              | Hate and Unfairness, Sexual, Violent, Self-Harm, XPIA, ECI (Text) | Groundedness (Text) | Protected Material (Text) | Hate and Unfairness, Sexual, Violent, Self-Harm, Protected Material (Image) |\n",
    "|---------------------|------------------------------------------------------------------|---------------------|----------------------------|----------------------------------------------------------------------------|\n",
    "| North Central US    | no                                                               | no                  | no                         | yes                                                                        |\n",
    "| East US 2           | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Sweden Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| US North Central    | yes                                                              | no                  | yes                        | yes                                                                        |\n",
    "| France Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Switzerland West    | yes                                                              | no                  | no                         | yes                                                                        |\n",
    "\n",
    "### Region support for adversarial simulation\n",
    "| Region            | Adversarial Simulation (Text) | Adversarial Simulation (Image) |\n",
    "|-------------------|-------------------------------|---------------------------------|\n",
    "| UK South          | yes                           | no                              |\n",
    "| East US 2         | yes                           | yes                             |\n",
    "| Sweden Central    | yes                           | yes                             |\n",
    "| US North Central  | yes                           | yes                             |\n",
    "| France Central    | yes                           | no                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è Pricing and billing\n",
    "- Effective 1/14/2025, Azure AI Safety Evaluations will no longer be free in public preview. It will be billed based on consumption as following:\n",
    "\n",
    "| Service Name              | Safety Evaluations       | Price Per 1K Tokens (USD) |\n",
    "|---------------------------|--------------------------|---------------------------|\n",
    "| Azure Machine Learning    | Input pricing for 3P     | $0.02                     |\n",
    "| Azure Machine Learning    | Output pricing for 3P    | $0.06                     |\n",
    "| Azure Machine Learning    | Input pricing for 1P     | $0.012                    |\n",
    "| Azure Machine Learning    | Output pricing for 1P    | $0.012                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator\n",
    ")\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Evaluators in Azure Cloud (azure.ai.evaluation.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_ai_project_conn_str = os.environ.get(\"AZURE_AI_PROJECT_CONN_STR\")\n",
    "subscription_id = azure_ai_project_conn_str.split(\";\")[1]\n",
    "resource_group_name = azure_ai_project_conn_str.split(\";\")[2]\n",
    "project_name = azure_ai_project_conn_str.split(\";\")[3]\n",
    "\n",
    "azure_ai_project_dict = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "azure_ai_project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=azure_ai_project_conn_str\n",
    ")\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/evaluate_test_data.jsonl\"\n",
    "output_path = \"data/informative_data_results_test.json\"\n",
    "\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk\n",
    "retrieval_evaluator = RetrievalEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "\n",
    "column_mapping = {\n",
    "    \"query\": \"${data.query}\",\n",
    "    \"ground_truth\": \"${data.ground_truth}\",\n",
    "    \"response\": \"${data.response}\",\n",
    "    \"context\": \"${data.context}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt flow service...\n",
      "Starting prompt flow service...Starting prompt flow service...\n",
      "\n",
      "Starting prompt flow service...\n",
      "Starting prompt flow service...\n",
      "Starting prompt flow service...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy, please check the logs for more details; traces might not be exported correctly.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ekbq98z8_20250226_103012_168278, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ekbq98z8_20250226_103012_168278/logs.txt\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy, please check the logs for more details; traces might not be exported correctly.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_o9y9ex2x_20250226_103012_178998, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_o9y9ex2x_20250226_103012_178998/logs.txt\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy, please check the logs for more details; traces might not be exported correctly.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_pbkt53bl_20250226_103012_177208, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_pbkt53bl_20250226_103012_177208/logs.txt\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy, please check the logs for more details; traces might not be exported correctly.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2mt_q42p_20250226_103012_176214, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_2mt_q42p_20250226_103012_176214/logs.txt\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy, please check the logs for more details; traces might not be exported correctly.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_jefbmxl2_20250226_103012_170277, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_jefbmxl2_20250226_103012_170277/logs.txt\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy. Kindly reminder: If you have previously upgraded the prompt flow package , please double-confirm that you have run '\u001b[1mpf service stop\u001b[0m' to stop the prompt flowservice before proceeding with the upgrade. Otherwise, you may encounter unexpected environmental issues or inconsistencies between the version of running prompt flow service and the local prompt flow version. Alternatively, you can use the '\u001b[1mpf upgrade\u001b[0m' command to proceed with the upgrade process for the prompt flow package.\n",
      "[2025-02-26 10:30:26 +0000][promptflow][WARNING] - Prompt flow service is not healthy, please check the logs for more details; traces might not be exported correctly.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-02-26 10:30:26 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_8_4x778f_20250226_103012_163198, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_8_4x778f_20250226_103012_163198/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 0.89 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 3.06 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 1.42 seconds. Estimated time for incomplete lines: 2.84 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 1.52 seconds. Estimated time for incomplete lines: 3.04 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 0.9 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 1.48 seconds. Estimated time for incomplete lines: 2.96 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 0.81 seconds. Estimated time for incomplete lines: 0.81 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 0.93 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 0.57 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 1.1 seconds.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-02-26 10:30:28 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 0.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-02-26 10:30:29 +0000  209770 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-02-26 10:30:29 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-02-26 10:30:29 +0000  209770 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-02-26 10:30:29 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-02-26 10:30:31 +0000  209770 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-02-26 10:30:31 +0000  209770 execution.bulk     INFO     Average execution time for completed lines: 1.49 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_name=f\"evaluation_cloud_{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    data=input_path,\n",
    "    evaluators={\n",
    "            \"retrieval\": retrieval_evaluator,\n",
    "            \"fluency\": fluency_evaluator,\n",
    "            \"groundedness\": groundedness_evaluator,\n",
    "            \"relevance\": relevance_evaluator,\n",
    "            \"coherence\": coherence_evaluator,\n",
    "            \"similarity\": similarity_evaluator\n",
    "\n",
    "        },\n",
    "    evaluator_config={\n",
    "        \"retrieval\": {\"column_mapping\": column_mapping},\n",
    "        \"relevance\": {\"column_mapping\": column_mapping},\n",
    "        \"similarity\": {\"column_mapping\": column_mapping},\n",
    "        \"groundedness\": {\"column_mapping\": column_mapping},\n",
    "        \"coherence\": {\"column_mapping\": column_mapping},\n",
    "        \"similarity\": {\"column_mapping\": column_mapping},\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project_dict,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Evaluators in Azure Cloud (azure.ai.projects.models.Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id for each evaluator can be found in your AI Studio registry - please see documentation for more information\n",
    "# init_params is the configuration for the model to use to perform the evaluation\n",
    "# data_mapping is used to map the output columns of your query to the names required by the evaluator\n",
    "# Evaluator parameter format - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#evaluator-parameter-format\n",
    "evaluators_cloud = {\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=F1ScoreEvaluator.id,\n",
    "    ),\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=RelevanceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=GroundednessEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    # \"retrieval\": EvaluatorConfiguration(\n",
    "    #     #from azure.ai.evaluation._evaluators._common.math import list_mean_nan_safe\\nModuleNotFoundError: No module named 'azure.ai.evaluation._evaluators._common.math'\n",
    "    #     id=RetrievalEvaluator.id,\n",
    "    #     #id=\"azureml://registries/azureml/models/Retrieval-Evaluator/versions/2\",\n",
    "    #     init_params={\"model_config\": model_config},\n",
    "    #     data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    # ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=CoherenceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=FluencyEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "     \"similarity\": EvaluatorConfiguration(\n",
    "        # currently bug in the SDK, please use the id below\n",
    "        #id=SimilarityEvaluator.id,\n",
    "        id=\"azureml://registries/azureml/models/Similarity-Evaluator/versions/3\",\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- The following code demonstrates how to upload the data for evaluation to your Azure AI project. Below we use evaluate_test_data.jsonl which exemplifies LLM-generated data in the query-response format expected by the Azure AI Evaluation SDK. For your use case, you should upload data in the same format, which can be generated using the Simulator from Azure AI Evaluation SDK.\n",
    "\n",
    "- Alternatively, if you already have an existing dataset for evaluation, you can use that by finding the link to your dataset in your registry or find the dataset ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload data for evaluation\n",
    "data_id, _ = azure_ai_project_client.upload_file(\"data/evaluate_test_data.jsonl\")\n",
    "# data_id = \"azureml://registries/<registry>/data/<dataset>/versions/<version>\"\n",
    "# To use an existing dataset, replace the above line with the following line\n",
    "# data_id = \"<dataset_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluators to Run\n",
    "- The code below demonstrates how to configure the evaluators you want to run. In this example, we use the F1ScoreEvaluator, RelevanceEvaluator and the ViolenceEvaluator, but all evaluators supported by Azure AI Evaluation are supported by cloud evaluation and can be configured here. You can either import the classes from the SDK and reference them with the .id property, or you can find the fully formed id of the evaluator in the AI Studio registry of evaluators, and use it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluation(\n",
    "    display_name=f\"evaluation_cloud_{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    description=\"Cloud Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators=evaluators_cloud,\n",
    ")\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_response = azure_ai_project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_status(project_client:AIProjectClient, evaluation_response_id:str):\n",
    "    with tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        if status == \"Queued\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Completed\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        while(pbar.n < 3):\n",
    "            pbar.update(1)\n",
    "        print(\"Operation Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_status(azure_ai_project_client, evaluation_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the evaluation result in Azure AI Foundry \n",
    "- After running the evaluation, you can check the evaluation results in Azure AI Foundry. You can find the evaluation results in the Evaluation tab of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation\n",
    "get_evaluation_response = azure_ai_project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", get_evaluation_response.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI Foundry Portal URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cloud Evaluation Result](./images/cloud_evaluation_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_aoai_sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
