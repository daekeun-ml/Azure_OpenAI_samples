{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule an Online Evaluation with Tracing and AI Inference SDK (Not Tested)\n",
    "- You can configure the RecurrenceTrigger based on the class definition here. The code below demonstrates how to configure the RecurrenceTrigger to run the evaluation every 24 hours. You can also configure the trigger to run at a different interval, or at a specific time of day. Check the Trace menu in the Azure AI Foundry to see the results of the evaluation.\n",
    "- reference: https://learn.microsoft.com/en-us/azure/ai-studio/how-to/online-evaluation\n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> You application insight need to be created in Azure AI Foundry to trace your generative AI application. <br>\n",
    "> Prior to setting up online evaluation, ensure you have first set up [tracing for your generative AI application using Azure AI Inference SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/trace-local-sdk?tabs=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator\n",
    ")\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_ai_project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ.get(\"AZURE_AI_PROJECT_CONN_STR\"),  # At the moment, it should be in the format \"<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>\" Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    ")\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_insights_connection_string = azure_ai_project_client.telemetry.get_connection_string()\n",
    "if not application_insights_connection_string:\n",
    "    print(\"Application Insights was not enabled for this project.\")\n",
    "    print(\"Enable it via the 'Tracing' tab in your Azure AI Foundry project page.\")\n",
    "    exit()\n",
    "\n",
    "from azure.core.settings import settings \n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/trace-local-sdk?tabs=python\n",
    "os.environ['AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED'] = 'true'\n",
    "settings.tracing_implementation = \"opentelemetry\" \n",
    "\n",
    "configure_azure_monitor(connection_string=application_insights_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor \n",
    "\n",
    "# Instrument AI Inference API to enable trace instrumentation for AI Inference\n",
    "AIInferenceInstrumentor().instrument() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initialized azure_ai_inference_client ===\n",
      "AZURE_AI_INFERENCE_ENDPOINT=https://aoai-services1.services.ai.azure.com/models\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "aoai_inference_endpoint = os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    azure_ai_inference_client = ChatCompletionsClient(\n",
    "        endpoint = aoai_inference_endpoint,\n",
    "        credential = AzureKeyCredential(aoai_api_key),\n",
    "    )\n",
    "\n",
    "    print(\"=== Initialized azure_ai_inference_client ===\")\n",
    "    print(f\"AZURE_AI_INFERENCE_ENDPOINT={aoai_inference_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "        \n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"What is the warranty period for Contoso products?\",\"context\":\"Customer service inquiry\",\"response\":\"The warranty period for Contoso products typically lasts for one year from the date of purchase, covering manufacturing defects and issues.\",\"ground_truth\":\"The warranty period for Contoso products is one year.\"}\n",
      "{\"query\":\"How can I track my order from Contoso?\",\"context\":\"Order tracking information\",\"response\":\"You can track your order from Contoso by visiting our website and entering your order number in the tracking section.\",\"ground_truth\":\"You can track your order by entering your order number on our website.\"}\n",
      "{\"query\":\"What should I do if my device is not charging?\",\"context\":\"Technical support question\",\"response\":\"If your device is not charging, first check the power source and cable. If those are fine, try resetting the device or contacting support for further assistance.\",\"ground_truth\":\"Check the power source and cable, then reset the device.\"}\n",
      "{\"query\":\"Can I return a product after opening it?\",\"context\":\"Return policy inquiry\",\"response\":\"Yes, you can return a product after opening it, but it must be in good condition and within the return period specified in our policy.\",\"ground_truth\":\"You can return an opened product if it's in good condition.\"}\n",
      "{\"query\":\"What types of payment does Contoso accept?\",\"context\":\"Payment options information\",\"response\":\"Contoso accepts various payment methods including credit cards, PayPal, and bank transfers for your convenience.\",\"ground_truth\":\"Contoso accepts credit cards, PayPal, and bank transfers.\"}\n",
      "{\"query\":\"How do I reset my Contoso device?\",\"context\":\"Device troubleshooting\",\"response\":\"To reset your Contoso device, locate the reset button, press and hold it for ten seconds, and then release it to initiate the reset process.\",\"ground_truth\":\"Press and hold the reset button for ten seconds.\"}\n",
      "{\"query\":\"What is the latest firmware version for my device?\",\"context\":\"Firmware update inquiry\",\"response\":\"You can check the latest firmware version for your device on the Contoso support page under the firmware updates section.\",\"ground_truth\":\"Check the support page for the latest firmware version.\"}\n",
      "{\"query\":\"How do I connect my device to Wi-Fi?\",\"context\":\"Device setup instructions\",\"response\":\"To connect your device to Wi-Fi, go to the settings menu, select Wi-Fi, choose your network, and enter the password when prompted.\",\"ground_truth\":\"Go to settings, select Wi-Fi, and enter the password.\"}\n",
      "{\"query\":\"What is the return policy for Contoso products?\",\"context\":\"Return policy details\",\"response\":\"Contoso's return policy allows returns within 30 days of purchase, provided the product is in its original packaging and condition.\",\"ground_truth\":\"Returns are allowed within 30 days if the product is in original condition.\"}\n",
      "{\"query\":\"How can I get technical support for my Contoso device?\",\"context\":\"Technical support options\",\"response\":\"You can get technical support for your Contoso device by calling our support hotline or visiting our website for live chat assistance.\",\"ground_truth\":\"Call the support hotline or use live chat on our website.\"}\n",
      "{\"query\":\"What accessories are compatible with my Contoso device?\",\"context\":\"Accessory compatibility inquiry\",\"response\":\"You can find a list of compatible accessories for your Contoso device on our website under the accessories section.\",\"ground_truth\":\"Check the accessories section on our website for compatibility.\"}\n",
      "{\"query\":\"How do I update my Contoso device?\",\"context\":\"Device update instructions\",\"response\":\"To update your Contoso device, go to the settings menu, select 'Software Update', and follow the prompts to install the latest version.\",\"ground_truth\":\"Go to settings, select 'Software Update', and follow the prompts.\"}\n",
      "{\"query\":\"What should I do if my device is overheating?\",\"context\":\"Device safety inquiry\",\"response\":\"If your device is overheating, turn it off immediately, unplug it, and allow it to cool down before using it again.\",\"ground_truth\":\"Turn off the device and let it cool down.\"}\n",
      "{\"query\":\"Can I extend the warranty on my Contoso product?\",\"context\":\"Warranty extension inquiry\",\"response\":\"Yes, you can extend the warranty on your Contoso product by purchasing an extended warranty plan through our website.\",\"ground_truth\":\"You can purchase an extended warranty plan on our website.\"}\n",
      "{\"query\":\"What is the process for getting a refund?\",\"context\":\"Refund process inquiry\",\"response\":\"To get a refund, please fill out the refund request form on our website and provide the necessary details about your purchase.\",\"ground_truth\":\"Fill out the refund request form on our website.\"}\n",
      "{\"query\":\"How do I clean my Contoso device?\",\"context\":\"Device maintenance tips\",\"response\":\"To clean your Contoso device, use a soft, lint-free cloth and a mild cleaning solution. Avoid using abrasive materials that could scratch the surface.\",\"ground_truth\":\"Use a soft cloth and mild cleaning solution to clean the device.\"}\n",
      "{\"query\":\"What are the shipping options available for Contoso products?\",\"context\":\"Shipping information\",\"response\":\"Contoso offers standard, expedited, and overnight shipping options for your orders, depending on your location and urgency.\",\"ground_truth\":\"Standard, expedited, and overnight shipping options are available.\"}\n",
      "{\"query\":\"How do I register my Contoso product?\",\"context\":\"Product registration instructions\",\"response\":\"To register your Contoso product, visit our website and fill out the product registration form with your purchase details.\",\"ground_truth\":\"Visit our website to fill out the product registration form.\"}\n",
      "{\"query\":\"What is the expected delivery time for my order?\",\"context\":\"Delivery time inquiry\",\"response\":\"The expected delivery time for your order is typically between 3 to 5 business days, depending on your location and shipping method selected.\",\"ground_truth\":\"Delivery usually takes 3 to 5 business days.\"}\n",
      "Usage Information:\n",
      "Completion Tokens: 1189\n",
      "Prompt Tokens: 898\n",
      "Total Tokens: 2087\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "NUM_SAMPLES = 20\n",
    "\n",
    "LANGUAGE = \"English\"\n",
    "\n",
    "topic = f\"\"\"\n",
    "Contoso Electronics call center QnA related expected spoken utterances for {LANGUAGE} languages. create {NUM_SAMPLES} lines of jsonl of the topic related text.\n",
    "\"\"\"\n",
    "\n",
    "example = \"\"\"\n",
    "Here is examples of the expected format:\n",
    "{\"query\":\"How do you create a run?\",\"context\":\"AML API only\",\"response\":\"To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\\\\n\\\\n```\\\\nfrom azureml.core import Experiment, Run\\\\nfrom azureml.core.workspace import Workspace\\\\n\\\\n# Define workspace and experiment\\\\nws = Workspace.from_config()\\\\nexp = Experiment(workspace=ws, name='my_experiment')\\\\n\\\\n# Create a new run\\\\nrun = exp.start_logging()\\\\n```\\\\n\\\\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "{\"query\":\"How do you log a model?\",\"context\":\"Logging can be done using any OSS Sdk\",\"response\":\"There are a few ways to log models in Azure Machine Learning. \\\\n\\\\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\\\\n\\\\n```python\\\\nfrom azureml.core import Model\\\\n\\\\nmodel_path = './outputs/my_model.pkl'\\\\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\\\\n```\\\\n\\\\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \\\\n\\\\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\\\\n\\\\n```python\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom azureml.core.run import Run\\\\n\\\\n# Initialize a run object\\\\nrun = Run.get_context()\\\\n\\\\n# Train your model\\\\nX_train, y_train = ...\\\\nlog_reg = LogisticRegression().fit(X_train, y_train)\\\\n\\\\n# Save the model to the Run object's outputs directory\\\\nmodel_path = 'outputs/model.pkl'\\\\njoblib.dump(value=log_reg, filename=model_path)\\\\n\\\\n# Log the model as a run artifact\\\\nrun.upload_file(name=model_path, path_or_stream=model_path)\\\\n```\\\\n\\\\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "{\"query\":\"What is the capital of France?\",\"context\":\"France is in Europe\",\"response\":\"Paris is the capital of France.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "Generate plain text sentences of #topic# related text to improve the recognition of domain-specific words and phrases.\n",
    "Domain-specific words can be uncommon or made-up words, but their pronunciation must be straightforward to be recognized. \n",
    "Use text data that's close to the expected spoken utterances. The nummber of utterances per line should be 1. \n",
    "jsonl format is required. use 'no' as number, 'query' as string, 'context' as string, 'response' as string, and 'ground_truth' as string.\n",
    "only include the lines as the result. Do not include ```jsonl, ``` and blank line in the result. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "#topic#: {topic}\n",
    "Example: {example}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = azure_ai_inference_client.complete(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "print(content)\n",
    "print(\"Usage Information:\")\n",
    "#print(f\"Cached Tokens: {response.usage.prompt_tokens_details.cached_tokens}\") #only o1 models support this\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up online evaluation schedule \n",
    "\n",
    "Evaluations are only supported in the same regions as AI-assisted risk and safety metrics. Please find the reference document here. (https://learn.microsoft.com/en-us/azure/ai-studio/how-to/online-evaluation?tabs=linux) \n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A new User-assigned Managed Identity in the same resource group and region. Make a note of the clientId; you'll need it later.\n",
    "- An Azure AI Hub in the same resource group and region.\n",
    "- An Azure AI project in this hub, see Create a project in Azure AI Foundry portal.\n",
    "- An Azure Monitor Application Insights resource created in Azure AI Foundry portal.\n",
    "- Navigate to the hub page in Azure portal and add Application Insights resource, see Update Azure Application Insights\n",
    "- Azure OpenAI Deployment with GPT model supporting chat completion, for example gpt-4.\n",
    "- Navigate to your Application Insights resource in the Azure portal and use the Access control (IAM) tab to add the Log Analytics Contributor role to the User-assigned Managed Identity you created previously.\n",
    "- [Attach the User-assigned Managed Identity to your project.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&tabs=cli#add-a-user-assigned-managed-identity-to-a-workspace-in-addition-to-a-system-assigned-identity)\n",
    "- Before attaching the User-assigned Managed Identity to your project, you also need to set the permission on the user assigned managed identity including keyvault access policies. - https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&tabs=cli#user-assigned-managed-identity\n",
    "- If you run into issues attaching the User-assigned Managed Identity to your project, you need to assign Key Vault Contributor role to the workspace (project) resource. \n",
    "- Navigate to your Azure AI Services in the Azure portal and use the Access control (IAM) tab to add the Cognitive Services OpenAI Contributor role to the User-assigned Managed Identity you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id for each evaluator can be found in your AI Studio registry - please see documentation for more information\n",
    "# init_params is the configuration for the model to use to perform the evaluation\n",
    "# data_mapping is used to map the output columns of your query to the names required by the evaluator\n",
    "# Evaluator parameter format - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#evaluator-parameter-format\n",
    "evaluators_cloud = {\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=RelevanceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=GroundednessEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    # \"retrieval\": EvaluatorConfiguration(\n",
    "    #     #from azure.ai.evaluation._evaluators._common.math import list_mean_nan_safe\\nModuleNotFoundError: No module named 'azure.ai.evaluation._evaluators._common.math'\n",
    "    #     #id=RetrievalEvaluator.id,\n",
    "    #     id=\"azureml://registries/azureml/models/Retrieval-Evaluator/versions/2\",\n",
    "    #     init_params={\"model_config\": model_config},\n",
    "    #     data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    # ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=CoherenceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=FluencyEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "     \"similarity\": EvaluatorConfiguration(\n",
    "        # currently bug in the SDK, please use the id below\n",
    "        #id=SimilarityEvaluator.id,\n",
    "        id=\"azureml://registries/azureml/models/Similarity-Evaluator/versions/3\",\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kusto_query = 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id'\n",
    "\n",
    "# AzureMSIClientId is the clientID of the User-assigned managed identity created during set-up - see documentation for how to find it\n",
    "properties = {\"AzureMSIClientId\": os.environ.get(\"AZURE_MSI_CLIENT_ID\")}\n",
    "\n",
    "service_name = \"evaluation_sdk_schedule\"\n",
    "\n",
    "# Your Application Insights resource ID\n",
    "# At the moment, it should be something in the format \"/subscriptions/<AzureSubscriptionId>/resourceGroups/<ResourceGroup>/providers/Microsoft.Insights/components/<ApplicationInsights>\"\"\n",
    "app_insights_resource_id = os.environ.get(\"APP_INSIGHTS_RESOURCE_ID\")\n",
    "\n",
    "# Connect to your Application Insights resource\n",
    "app_insights_config = ApplicationInsightsConfiguration(\n",
    "    resource_id=app_insights_resource_id, query=kusto_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted the online evaluation schedule creation request - evaluation_sdk_schedule, currently in Creating state.\n"
     ]
    }
   ],
   "source": [
    "# Frequency to run the schedule\n",
    "recurrence_trigger = RecurrenceTrigger(frequency=\"hour\", interval=1)\n",
    "\n",
    "# Configure the online evaluation schedule\n",
    "evaluation_schedule = EvaluationSchedule(\n",
    "    data=app_insights_config,\n",
    "    evaluators=evaluators_cloud,\n",
    "    trigger=recurrence_trigger,\n",
    "    description=f\"scheduled evaluation\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "# Create the online evaluation schedule\n",
    "created_evaluation_schedule = azure_ai_project_client.evaluations.create_or_replace_schedule(service_name, evaluation_schedule)\n",
    "print(\n",
    "    f\"Successfully submitted the online evaluation schedule creation request - {created_evaluation_schedule.name}, currently in {created_evaluation_schedule.provisioning_state} state.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'evaluation_sdk_schedule', 'description': 'scheduled evaluation', 'isEnabled': True, 'provisioningState': 'Succeeded', 'data': {'type': 'AppInsights', 'resourceId': 'your-app-insights-resource-id', 'query': 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id', 'connectionString': '', 'serviceName': '', 'id': ''}, 'tags': {}, 'properties': {'AzureMSIClientId': 'your-azure-msi-client-id'}, 'evaluators': {'relevance': {'id': 'azureml://registries/azureml/models/Relevance-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'groundedness': {'id': 'azureml://registries/azureml/models/Groundedness-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'coherence': {'id': 'azureml://registries/azureml/models/Coherence-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}, 'fluency': {'id': 'azureml://registries/azureml/models/Fluency-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'similarity': {'id': 'azureml://registries/azureml/models/Similarity-Evaluator/versions/3', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}}, 'trigger': {'type': 'Recurrence', 'frequency': 'Hour', 'interval': 1}, 'systemData': {'createdBy': '', 'modifiedBy': ''}}\n",
      "{'name': 'evaluation_sdk_schedule', 'description': 'scheduled evaluation', 'isEnabled': True, 'provisioningState': 'Succeeded', 'data': {'type': 'AppInsights', 'resourceId': 'your-app-insights-resource-id', 'query': 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id', 'connectionString': '', 'serviceName': '', 'id': ''}, 'tags': {}, 'properties': {'AzureMSIClientId': 'your-azure-msi-client-id'}, 'evaluators': {'relevance': {'id': 'azureml://registries/azureml/models/Relevance-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'groundedness': {'id': 'azureml://registries/azureml/models/Groundedness-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'coherence': {'id': 'azureml://registries/azureml/models/Coherence-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}, 'fluency': {'id': 'azureml://registries/azureml/models/Fluency-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'similarity': {'id': 'azureml://registries/azureml/models/Similarity-Evaluator/versions/3', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '1gLxmgk6o5ZwShT5PnrtSU4c7EustWSi9sBpCO50HlJgl9H2yuxlJQQJ99BBACfhMk5XJ3w3AAAAACOGvg2a', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}}, 'trigger': {'type': 'Recurrence', 'frequency': 'Hour', 'interval': 1}, 'systemData': {'createdBy': '', 'modifiedBy': ''}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluation_schedule = azure_ai_project_client.evaluations.get_schedule(service_name)\n",
    "print(evaluation_schedule)\n",
    "\n",
    "# Sample for list evaluation schedules\n",
    "for evaluation_schedule in azure_ai_project_client.evaluations.list_schedule():\n",
    "    print(evaluation_schedule)\n",
    "\n",
    "# Sample for disable an evaluation schedule with name\n",
    "# project_client.evaluations.disable_schedule(service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.evaluation_sdk_schedule [IsEnabled: True]\n",
      "Total evaluation schedules: 1\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for evaluation_schedule in azure_ai_project_client.evaluations.list_schedule():\n",
    "    count += 1\n",
    "    print(f\"{count}.{evaluation_schedule.name} \"\n",
    "    f\"[IsEnabled: {evaluation_schedule.is_enabled}]\")\n",
    "    print(f\"Total evaluation schedules: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable (soft-delete) online evaluation schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_service_name = service_name\n",
    "# azure_ai_project_client.evaluations.disable_schedule(target_service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_aoai_sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
