{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Evaluators with the Azure AI Evaluation SDK\n",
    "The following sample shows the basic way to evaluate a Generative AI application in your development environment with the Azure AI evaluation SDK.\n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> Please check the reference document before you get started - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk\n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "For a dependency installation, run the code below to install the packages required to run it. \n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Set up your environment\n",
    "Git clone the repository to your local machine. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/hyogrin/Azure_OpenAI_samples.git\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## ðŸ”¨ Current Support and Limitations (as of 2025-01-12) \n",
    "- Check the region support for the Azure AI Evaluation SDK. https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#region-support\n",
    "\n",
    "| Region | Hate and Unfairness, Sexual, Violent, Self-Harm, XPIA | Groundedness Pro | Protected Material |\n",
    "| - | - | - | - |\n",
    "| UK South | Will be deprecated 12/1/24 | no | no |\n",
    "| East US 2 | yes | yes | yes |\n",
    "| Sweden Central | yes | yes | no |\n",
    "| US North Central | yes | no | no |\n",
    "| France Central | yes | no | no |\n",
    "| Switzerland West | yes | no | no |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    IndirectAttackEvaluator,\n",
    ")\n",
    "from azure.ai.evaluation.simulator import (\n",
    "    AdversarialSimulator,\n",
    "    AdversarialScenario,\n",
    "    AdversarialScenarioJailbreak,\n",
    "    IndirectAttackSimulator,\n",
    ")\n",
    "from openai import AzureOpenAI\n",
    "from model_endpoint import ModelEndpoint\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initialize Azure AI project and Azure OpenAI conncetion with your environment variables\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "azure_openai_deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "azure_openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "azure_openai_api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "conversation_str =  \"\"\"{\"messages\": [ { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} ] }\"\"\" \n",
    "conversation = json.loads(conversation_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª AI-assisted ContentSafety evaluator\n",
    "- Combines all the safety evaluators - ViolenceEvaluator, SexualEvaluator, SelfHarmEvaluator and HateUnfairnessEvaluator for a single output of combined metrics for query and response pairs\n",
    "- Evaluating direct attack is a comparative measurement using the content safety evaluators as a control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentsafety_eval = ContentSafetyEvaluator(credential=credential, azure_ai_project=azure_ai_project)\n",
    "\n",
    "uery_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\", # optional\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "contentsafety_score = contentsafety_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(contentsafety_score)\n",
    "\n",
    "contentsafety_conv_score = contentsafety_eval(conversation=conversation)\n",
    "print(contentsafety_conv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª AI-assisted Indirect Attack evaluator\n",
    "- Evaluates the indirect attack score for a given query and response or a multi-turn conversation, with reasoning.\n",
    "- Indirect attacks, also known as cross-domain prompt injected attacks (XPIA), are when jailbreak attacks are injected into the context of a document or source that may result in an altered, unexpected behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirect_attack_eval = IndirectAttackEvaluator(credential=credential, azure_ai_project=azure_ai_project)\n",
    "\n",
    "uery_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\", # optional\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "indirect_attack_score = indirect_attack_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(indirect_attack_score)\n",
    "\n",
    "indirect_attack_cov_score = indirect_attack_eval(conversation=conversation)\n",
    "print(indirect_attack_cov_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Use AdversarialSimulator to generate abnormal contents\n",
    "- Test that the Protected Material (i.e. copyrighted content or material) is not being generated by your generative AI applications. The following example uses an AdversarialSimulator paired with a protected content scenario to prompt your model to respond with material that is protected by intellectual property laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "    deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "    endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    # Get a client handle for the model\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=azure_openai_endpoint,\n",
    "        api_version=azure_openai_api_version,\n",
    "        api_key=azure_openai_key,\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    # change this to return the response from your application\n",
    "    return message[\"content\"]\n",
    "\n",
    "\n",
    "async def callback(\n",
    "    messages: Dict[str, List[Dict]],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # Get the last message from the user\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "\n",
    "    # Call the model\n",
    "    response = call_to_your_ai_application(query)\n",
    "\n",
    "    formatted_response = formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": \"\",\n",
    "    }\n",
    "    \n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the adversarial simulator\n",
    "protected_material_simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "outputs = await protected_material_simulator(\n",
    "    scenario=AdversarialScenario.ADVERSARIAL_CONTENT_PROTECTED_MATERIAL,\n",
    "    max_conversation_turns=1,  # define the number of conversation turns\n",
    "    max_simulation_results=3,  # define the number of simulation results\n",
    "    target=callback,  # define the target model callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually convert the data to JSON lines format\n",
    "result = \"\\n\".join([json.dumps(item) for item in outputs])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the adversarial simulator\n",
    "qna_simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "outputs = await qna_simulator(\n",
    "    scenario=AdversarialScenario.ADVERSARIAL_QA,\n",
    "    max_conversation_turns=1,  # define the number of conversation turns\n",
    "    max_simulation_results=3,  # define the number of simulation results\n",
    "    target=callback,  # define the target model callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually convert the data to JSON lines format\n",
    "result = \"\\n\".join([json.dumps(item) for item in outputs])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
