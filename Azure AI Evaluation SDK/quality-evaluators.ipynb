{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Evaluators with the Azure AI Evaluation SDK\n",
    "The following sample shows the basic way to evaluate a Generative AI application in your development environment with the Azure AI evaluation SDK.\n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> Please check the reference document before you get started - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk\n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "For a dependency installation, run the code below to install the packages required to run it. \n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Set up your environment\n",
    "Git clone the repository to your local machine. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/hyogrin/Azure_OpenAI_samples.git\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## ðŸ”¨ Current Support and Limitations (as of 2025-01-12) \n",
    "- Check the region support for the Azure AI Evaluation SDK. https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#region-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator\n",
    ")\n",
    "from model_endpoint import ModelEndpoint\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initialize Azure AI project and Azure OpenAI conncetion with your environment variables\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "conversation_str =  \"\"\"{\"messages\": [ { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} ] }\"\"\" \n",
    "conversation = json.loads(conversation_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª AI-assisted Groundedness evaluator\n",
    "- Prompt-based groundedness using your own model deployment to output a score and an explanation for the score is currently supported in all regions.\n",
    "- Groundedness Pro evaluator leverages Azure AI Content Safety Service (AACS) via integration into the Azure AI Foundry evaluations. No deployment is required, as a back-end service will provide the models for you to output a score and reasoning. Groundedness Pro is currently supported in the East US 2 and Sweden Central regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class GroundednessProEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'groundedness_reason': 'The response accurately and completely answers the query based on the provided context.'}\n",
      "{'groundedness_pro_label': True, 'groundedness_pro_reason': 'All Contents are grounded'}\n",
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'evaluation_per_turn': {'groundedness': [5.0, 5.0], 'gpt_groundedness': [5.0, 5.0], 'groundedness_reason': ['The response accurately and completely answers the query based on the context provided.', 'The response is fully correct and complete, directly addressing the query with precise information from the context.']}}\n"
     ]
    }
   ],
   "source": [
    "# Initialzing Groundedness and Groundedness Pro evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "# No need to set the model_config for GroundednessProEvaluator\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\", # optional\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "# Running Groundedness Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_score)\n",
    "\n",
    "# Running Groundedness Pro Evaluator on a query and response pair\n",
    "groundedness_pro_score = groundedness_pro_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_pro_score)\n",
    "\n",
    "# input conversation result for the groundedness evaluator\n",
    "groundedness_conv_score = groundedness_eval(conversation=conversation)\n",
    "print(groundedness_conv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª AI-assisted RelevanceEvaluator\n",
    "- Relevance refers to how effectively a response addresses a question. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    #context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "relevance_score = relevance_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(relevance_score)\n",
    "\n",
    "# input conversation result\n",
    "relevance_conv_score = relevance_eval(conversation=conversation)\n",
    "print(relevance_conv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª AI-assisted CoherenceEvaluator\n",
    "- Coherence refers to the logical and orderly presentation of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought. A coherent answer directly addresses the question with clear connections between sentences and paragraphs, using appropriate transitions and a logical sequence of ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    #context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "relevance_score = relevance_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(relevance_score)\n",
    "\n",
    "relevance_conv_score = relevance_eval(conversation=conversation)\n",
    "print(relevance_conv_score)\n",
    "\n",
    "coherence_score = coherence_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(coherence_score)\n",
    "\n",
    "# input conversation result\n",
    "coherence_conv_score = coherence_eval(conversation=conversation)\n",
    "print(coherence_conv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª AI-assisted FluencyEvaluator\n",
    "- Fluency refers to the effectiveness and clarity of written communication, focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, and overall readability. It assesses how smoothly ideas are conveyed and how easily the text can be understood by the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "\n",
    "query_response = dict(\n",
    "    #query=\"Which tent is the most waterproof?\",\n",
    "    #context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "fluency_score = fluency_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(fluency_score)\n",
    "\n",
    "# input conversation result\n",
    "fluency_conv_score = fluency_eval(conversation=conversation)\n",
    "print(fluency_conv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª AI-assisted SimilarityEvaluator\n",
    "- The similarity metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the definition and grading rubrics below.\n",
    "- The recommended scenario is NLP tasks with a user query. Use it when you want an objective evaluation of an AI model's performance, particularly in text generation tasks where you have access to ground truth responses. Similarity enables you to assess the generated text's semantic alignment with the desired content, helping to gauge the model's quality and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    #context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    ground_truth=\"The Alpine Explorer tent has a rainfly waterproof rating of 2000mm\"\n",
    ")\n",
    "\n",
    "similarity_score = similarity_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(similarity_score)\n",
    "\n",
    "# input conversation Not support\n",
    "# similarity_conv_score = similarity_eval(conversation=conversation)\n",
    "# print(similarity_conv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Evaluators with target model endpoint\n",
    "\n",
    "- We will use Evaluation SDK. It requires a target Application or python Function, which handles a call to LLMs and retrieve responses.\n",
    "- In the notebook, we will use an Application Target ModelEndpoint to get answers against provided question prompts.\n",
    "- multiple model endpoints example - https://github.com/Azure-Samples/azureai-samples/blob/main/scenarios/evaluate/Supported_Evaluation_Targets/Evaluate_Base_Model_Endpoint/model_endpoints.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ContentSafetyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ViolenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SexualEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SelfHarmEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class HateUnfairnessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'azure_endpoint': 'https://aoai-services1.services.ai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o', 'api_version': '2024-12-01-preview', 'type': 'azure_openai'}\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=model_endpoint_modelendpoint_xgvkk381_20250112_143348_269443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-12 14:33:48 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run model_endpoint_modelendpoint_xgvkk381_20250112_143348_269443, log path: /home/vscode/.promptflow/.runs/model_endpoint_modelendpoint_xgvkk381_20250112_143348_269443/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-12 14:33:55 +0000 2389289 execution.bulk     INFO     Process 2389310 terminated.\n",
      "2025-01-12 14:33:55 +0000 2389289 execution.bulk     WARNING  Process 2389316 had been terminated.\n",
      "2025-01-12 14:33:55 +0000 2389289 execution.bulk     WARNING  Process 2389327 had been terminated.\n",
      "2025-01-12 14:33:55 +0000 2389289 execution.bulk     WARNING  Process 2389332 had been terminated.\n",
      "2025-01-12 14:33:48 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:33:48 +0000 2388621 execution          WARNING  Starting run without column mapping may lead to unexpected results. Please consult the following documentation for more information: https://aka.ms/pf/column-mapping\n",
      "2025-01-12 14:33:48 +0000 2388621 execution.bulk     INFO     Set process count to 4 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 4}.\n",
      "2025-01-12 14:33:50 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(2389327)-Line number(1) start execution.\n",
      "2025-01-12 14:33:50 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(2389316)-Line number(3) start execution.\n",
      "2025-01-12 14:33:50 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(2389332)-Line number(2) start execution.\n",
      "2025-01-12 14:33:50 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(2389310)-Line number(0) start execution.\n",
      "2025-01-12 14:33:51 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(2389310)-Line number(0) completed.\n",
      "2025-01-12 14:33:51 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(2389316)-Line number(3) completed.\n",
      "2025-01-12 14:33:52 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:52 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.0 seconds. Estimated time for incomplete lines: 4.0 seconds.\n",
      "2025-01-12 14:33:54 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(2389327)-Line number(1) completed.\n",
      "2025-01-12 14:33:54 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:54 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.0 seconds. Estimated time for incomplete lines: 2.0 seconds.\n",
      "2025-01-12 14:33:55 +0000 2388621 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(2389332)-Line number(2) completed.\n",
      "2025-01-12 14:33:55 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:55 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.75 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-01-12 14:33:55 +0000 2388621 execution.bulk     INFO     The thread monitoring the process [2389310-ForkProcess-4:1] will be terminated.\n",
      "2025-01-12 14:33:55 +0000 2388621 execution.bulk     INFO     The thread monitoring the process [2389316-ForkProcess-4:2] will be terminated.\n",
      "2025-01-12 14:33:55 +0000 2388621 execution.bulk     INFO     The thread monitoring the process [2389327-ForkProcess-4:3] will be terminated.\n",
      "2025-01-12 14:33:55 +0000 2388621 execution.bulk     INFO     The thread monitoring the process [2389332-ForkProcess-4:4] will be terminated.\n",
      "2025-01-12 14:33:55 +0000 2389310 execution.bulk     INFO     The process [2389310] has received a terminate signal.\n",
      "2025-01-12 14:33:55 +0000 2389316 execution.bulk     INFO     The process [2389316] has received a terminate signal.\n",
      "2025-01-12 14:33:55 +0000 2389332 execution.bulk     INFO     The process [2389332] has received a terminate signal.\n",
      "2025-01-12 14:33:55 +0000 2389327 execution.bulk     INFO     The process [2389327] has received a terminate signal.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"model_endpoint_modelendpoint_xgvkk381_20250112_143348_269443\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:48.268451+00:00\"\n",
      "Duration: \"0:00:08.279013\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/model_endpoint_modelendpoint_xgvkk381_20250112_143348_269443\"\n",
      "\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_f98plaxd_20250112_143356_651383\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ta6twnd2_20250112_143356_646160\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ya03cfgc_20250112_143356_654429\n",
      "Prompt flow service has started...\n",
      "Prompt flow service has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-12 14:33:56 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-01-12 14:33:56 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-01-12 14:33:56 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-01-12 14:33:56 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-01-12 14:33:56 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-01-12 14:33:56 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_dpcw9ucu_20250112_143356_678799, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_dpcw9ucu_20250112_143356_678799/logs.txt\n",
      "[2025-01-12 14:33:56 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-01-12 14:33:56 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ya03cfgc_20250112_143356_654429, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ya03cfgc_20250112_143356_654429/logs.txt\n",
      "[2025-01-12 14:33:56 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-01-12 14:33:56 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kymyf07u_20250112_143356_661411, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kymyf07u_20250112_143356_661411/logs.txt\n",
      "[2025-01-12 14:33:56 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ta6twnd2_20250112_143356_646160, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ta6twnd2_20250112_143356_646160/logs.txt\n",
      "[2025-01-12 14:33:56 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_f98plaxd_20250112_143356_651383, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_f98plaxd_20250112_143356_651383/logs.txt\n",
      "[2025-01-12 14:33:56 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_q78a6q_r_20250112_143356_659462, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_q78a6q_r_20250112_143356_659462/logs.txt\n",
      "[2025-01-12 14:33:56 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_hwigmqnu_20250112_143356_682275, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_hwigmqnu_20250112_143356_682275/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kymyf07u_20250112_143356_661411\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_dpcw9ucu_20250112_143356_678799\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_q78a6q_r_20250112_143356_659462\n",
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_hwigmqnu_20250112_143356_682275\n",
      "2025-01-12 14:33:57 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:33:58 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:58 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.4 seconds. Estimated time for incomplete lines: 0.8 seconds.\n",
      "2025-01-12 14:33:58 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:58 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.28 seconds. Estimated time for incomplete lines: 0.28 seconds.\n",
      "2025-01-12 14:33:58 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:58 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.21 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_dpcw9ucu_20250112_143356_678799\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:56.667019+00:00\"\n",
      "Duration: \"0:00:01.654264\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_dpcw9ucu_20250112_143356_678799\"\n",
      "\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.8 seconds. Estimated time for incomplete lines: 5.4 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.98 seconds. Estimated time for incomplete lines: 5.94 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 1.86 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.01 seconds. Estimated time for incomplete lines: 2.02 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.01 seconds. Estimated time for incomplete lines: 6.03 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.64 seconds. Estimated time for incomplete lines: 0.64 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.0 seconds. Estimated time for incomplete lines: 6.0 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 2.06 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.74 seconds. Estimated time for incomplete lines: 0.74 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.52 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-01-12 14:33:57 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.8 seconds. Estimated time for incomplete lines: 5.4 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 1.86 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.64 seconds. Estimated time for incomplete lines: 0.64 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.52 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_q78a6q_r_20250112_143356_659462\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:56.652304+00:00\"\n",
      "Duration: \"0:00:02.851196\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_q78a6q_r_20250112_143356_659462\"\n",
      "\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 2.2 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.27 seconds. Estimated time for incomplete lines: 6.81 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.77 seconds. Estimated time for incomplete lines: 0.77 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.76 seconds. Estimated time for incomplete lines: 0.76 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.15 seconds. Estimated time for incomplete lines: 2.3 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.79 seconds. Estimated time for incomplete lines: 0.79 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.67 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-01-12 14:33:57 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.98 seconds. Estimated time for incomplete lines: 5.94 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.01 seconds. Estimated time for incomplete lines: 2.02 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.74 seconds. Estimated time for incomplete lines: 0.74 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ya03cfgc_20250112_143356_654429\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:56.646313+00:00\"\n",
      "Duration: \"0:00:03.686433\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ya03cfgc_20250112_143356_654429\"\n",
      "\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.76 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-01-12 14:33:57 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.01 seconds. Estimated time for incomplete lines: 6.03 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.03 seconds. Estimated time for incomplete lines: 2.06 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.77 seconds. Estimated time for incomplete lines: 0.77 seconds.\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.67 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_f98plaxd_20250112_143356_651383\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:56.642311+00:00\"\n",
      "Duration: \"0:00:03.772250\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_f98plaxd_20250112_143356_651383\"\n",
      "\n",
      "2025-01-12 14:33:57 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.27 seconds. Estimated time for incomplete lines: 6.81 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.15 seconds. Estimated time for incomplete lines: 2.3 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.79 seconds. Estimated time for incomplete lines: 0.79 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kymyf07u_20250112_143356_661411\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:56.651233+00:00\"\n",
      "Duration: \"0:00:03.799147\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kymyf07u_20250112_143356_661411\"\n",
      "\n",
      "2025-01-12 14:33:57 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 1 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 2.0 seconds. Estimated time for incomplete lines: 6.0 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 2 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 2.2 seconds.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Finished 3 / 4 lines.\n",
      "2025-01-12 14:33:59 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.76 seconds. Estimated time for incomplete lines: 0.76 seconds.\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:34:00 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 0.76 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_hwigmqnu_20250112_143356_682275\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:56.673251+00:00\"\n",
      "Duration: \"0:00:03.826338\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_hwigmqnu_20250112_143356_682275\"\n",
      "\n",
      "2025-01-12 14:34:48 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:34:48 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 12.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-01-12 14:33:57 +0000 2388621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-01-12 14:34:48 +0000 2388621 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-01-12 14:34:48 +0000 2388621 execution.bulk     INFO     Average execution time for completed lines: 12.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ta6twnd2_20250112_143356_646160\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-01-12 14:33:56.636340+00:00\"\n",
      "Duration: \"0:00:51.769941\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ta6twnd2_20250112_143356_646160\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"content_safety\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:51.769941\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ta6twnd2_20250112_143356_646160\"\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.686433\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ya03cfgc_20250112_143356_654429\"\n",
      "    },\n",
      "    \"retrieval_score\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.772250\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_f98plaxd_20250112_143356_651383\"\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.826338\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_hwigmqnu_20250112_143356_682275\"\n",
      "    },\n",
      "    \"groundedness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.799147\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kymyf07u_20250112_143356_661411\"\n",
      "    },\n",
      "    \"fluency\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:02.851196\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_q78a6q_r_20250112_143356_659462\"\n",
      "    },\n",
      "    \"similarity\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.654264\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_similarity_asyncsimilarityevaluator_dpcw9ucu_20250112_143356_678799\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "retrieval_evaluator = RetrievalEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "content_safety_evaluator = ContentSafetyEvaluator(\n",
    "    azure_ai_project=azure_ai_project, credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/data/data.jsonl\"\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"Eval-Run-\" + \"-\" + model_config[\"azure_deployment\"].title(),\n",
    "    data=path,\n",
    "    target=ModelEndpoint(model_config),\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"retrieval_score\": retrieval_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${target.response}\"}},\n",
    "        \"coherence\": {\"column_mapping\": {\"response\": \"${target.response}\", \"query\": \"${data.query}\"}},\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\", \"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        \"retrieval_score\": {\n",
    "            \"column_mapping\": {\"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${target.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"query\": \"${data.query}\",\n",
    "            }\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\"}\n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Evaluators in Azure Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ.get(\"AZURE_AI_PROJECT_CONN_STR\"),  # At the moment, it should be in the format \"<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>\" Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id for each evaluator can be found in your AI Studio registry - please see documentation for more information\n",
    "# init_params is the configuration for the model to use to perform the evaluation\n",
    "# data_mapping is used to map the output columns of your query to the names required by the evaluator\n",
    "evaluators_cloud = {\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=F1ScoreEvaluator.id,\n",
    "    ),\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=RelevanceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=GroundednessEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    # module not found error\n",
    "    # \"retrieval_score\": EvaluatorConfiguration(\n",
    "    #     id=RetrievalEvaluator.id,\n",
    "    #     init_params={\"model_config\": model_config},\n",
    "    #     data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\"},\n",
    "    # ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=CoherenceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=FluencyEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "     \"similarity\": EvaluatorConfiguration(\n",
    "        id=SimilarityEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- The following code demonstrates how to upload the data for evaluation to your Azure AI project. Below we use evaluate_test_data.jsonl which exemplifies LLM-generated data in the query-response format expected by the Azure AI Evaluation SDK. For your use case, you should upload data in the same format, which can be generated using the Simulator from Azure AI Evaluation SDK.\n",
    "\n",
    "- Alternatively, if you already have an existing dataset for evaluation, you can use that by finding the link to your dataset in your registry or find the dataset ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload data for evaluation\n",
    "data_id, _ = project_client.upload_file(\"data/evaluate_test_data.jsonl\")\n",
    "# data_id = \"azureml://registries/<registry>/data/<dataset>/versions/<version>\"\n",
    "# To use an existing dataset, replace the above line with the following line\n",
    "# data_id = \"<dataset_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluators to Run\n",
    "- The code below demonstrates how to configure the evaluators you want to run. In this example, we use the F1ScoreEvaluator, RelevanceEvaluator and the ViolenceEvaluator, but all evaluators supported by Azure AI Evaluation are supported by cloud evaluation and can be configured here. You can either import the classes from the SDK and reference them with the .id property, or you can find the fully formed id of the evaluator in the AI Studio registry of evaluators, and use it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud Evaluation\",\n",
    "    description=\"Cloud Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators=evaluators_cloud,\n",
    ")\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Created evaluation, evaluation ID:  8f3a5fb2-58a2-481a-b5f7-a4718df737eb\n",
      "Evaluation status:  Starting\n",
      "AI Foundry Portal URI:  https://ai.azure.com/build/evaluation/8f3a5fb2-58a2-481a-b5f7-a4718df737eb?wsid=/subscriptions/3d4d3dd0-79d4-40cf-a94e-b4154812c6ca/resourceGroups/AOAI-group3/providers/Microsoft.MachineLearningServices/workspaces/aoai-pjt1\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation\n",
    "get_evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", get_evaluation_response.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI Foundry Portal URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_status(project_client:AIProjectClient, evaluation_response_id:str):\n",
    "    with tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        if status == \"Queued\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Completed\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        while(pbar.n < 3):\n",
    "            pbar.update(1)\n",
    "        print(\"Operation Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status:  33%|###3      | 1/3 [00:00<00:00,  6.83step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Queued\n",
      "Current Status: Queued\n",
      "Current Status: Queued\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status:  67%|######6   | 2/3 [00:30<00:17, 17.92s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Running\n",
      "Current Status: Finalizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status: 100%|##########| 3/3 [01:11<00:00, 23.97s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monitor_status(project_client, evaluation_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate in the Cloud on a Schedule with Online Evaluation\n",
    "- You can configure the RecurrenceTrigger based on the class definition here. The code below demonstrates how to configure the RecurrenceTrigger to run the evaluation every 24 hours. You can also configure the trigger to run at a different interval, or at a specific time of day. Check the Trace menu in the Azure AI Foundry to see the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Application Insights resource ID\n",
    "# At the moment, it should be something in the format \"/subscriptions/<AzureSubscriptionId>/resourceGroups/<ResourceGroup>/providers/Microsoft.Insights/components/<ApplicationInsights>\"\"\n",
    "app_insights_resource_id = os.environ.get(\"APP_INSIGHTS_RESOURCE_ID\")\n",
    "\n",
    "# Name of your generative AI application (will be available in trace data in Application Insights)\n",
    "service_name = os.environ.get(\"SERVICE_NAME\")\n",
    "\n",
    "# Name of your online evaluation schedule\n",
    "evaluation_name = os.environ.get(\"EVALUATION_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kusto_query = 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id'\n",
    "\n",
    "# AzureMSIClientId is the clientID of the User-assigned managed identity created during set-up - see documentation for how to find it\n",
    "properties = {\"AzureMSIClientId\": \"your_client_id\"}\n",
    "\n",
    "# Connect to your Application Insights resource\n",
    "app_insights_config = ApplicationInsightsConfiguration(\n",
    "    resource_id=app_insights_resource_id, query=kusto_query, service_name=service_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted the online evaluation schedule creation request - daily-evaluation, currently in Creating state.\n"
     ]
    }
   ],
   "source": [
    "# Frequency to run the schedule\n",
    "recurrence_trigger = RecurrenceTrigger(frequency=\"day\", interval=1)\n",
    "\n",
    "# Configure the online evaluation schedule\n",
    "evaluation_schedule = EvaluationSchedule(\n",
    "    data=app_insights_config,\n",
    "    evaluators=evaluators_cloud,\n",
    "    trigger=recurrence_trigger,\n",
    "    description=f\"{service_name} evaluation schedule\",\n",
    "    properties=properties,\n",
    ")\n",
    "\n",
    "# Create the online evaluation schedule\n",
    "created_evaluation_schedule = project_client.evaluations.create_or_replace_schedule(service_name, evaluation_schedule)\n",
    "print(\n",
    "    f\"Successfully submitted the online evaluation schedule creation request - {created_evaluation_schedule.name}, currently in {created_evaluation_schedule.provisioning_state} state.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
